@article{https://doi.org/10.1002/smr.2571,
author = {Zhang, Huangzhao and Lu, Shuai and Li, Zhuo and Jin, Zhi and Ma, Lei and Liu, Yang and Li, Ge},
title = {CodeBERT-Attack: Adversarial attack against source code deep learning models via pre-trained model},
journal = {Journal of Software: Evolution and Process},
volume = {36},
number = {3},
pages = {e2571},
keywords = {black-box adversarial attack, pre-trained model, source code classification},
doi = {https://doi.org/10.1002/smr.2571},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2571},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2571},
abstract = {Abstract Over the past few years, the software engineering (SE) community has widely employed deep learning (DL) techniques in many source code processing tasks. Similar to other domains like computer vision and natural language processing (NLP), the state-of-the-art DL techniques for source code processing can still suffer from adversarial vulnerability, where minor code perturbations can mislead a DL model's inference. Efficiently detecting such vulnerability to expose the risks at an early stage is an essential step and of great importance for further enhancement. This paper proposes a novel black-box effective and high-quality adversarial attack method, namely CodeBERT-Attack (CBA), based on the powerful large pre-trained model (i.e., CodeBERT) for DL models of source code processing. CBA locates the vulnerable positions through masking and leverages the power of CodeBERT to generate textual preserving perturbations. We turn CodeBERT against DL models and further fine-tuned CodeBERT models for specific downstream tasks, and successfully mislead these victim models to erroneous outputs. In addition, taking the power of CodeBERT, CBA is capable of effectively generating adversarial examples that are less perceptible to programmers. Our in-depth evaluation on two typical source code classification tasks (i.e., functionality classification and code clone detection) against the most widely adopted LSTM and the powerful fine-tuned CodeBERT models demonstrate the advantages of our proposed technique in terms of both effectiveness and efficiency. Furthermore, our results also show (1) that pre-training may help CodeBERT gain resilience against perturbations further, and (2) certain pre-training tasks may be beneficial for adversarial robustness.},
year = {2024}
}
@article{https://doi.org/10.1002/stvr.1889,
author = {Zheng, Xuedan and Jiang, Mingyue and Quan Zhou, Zhi},
title = {Boosting Metamorphic Relation Prediction via Code Representation Learning: An Empirical Study},
journal = {Software Testing, Verification and Reliability},
volume = {34},
number = {6},
pages = {e1889},
keywords = {deep learning, predicting metamorphic relation, source code representation},
doi = {https://doi.org/10.1002/stvr.1889},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1889},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1889},
note = {e1889 stvr.1889},
abstract = {Abstract Metamorphic testing (MT) is an effective testing technique having a broad range of applications. One key task for MT is the identification of metamorphic relations (MRs), which is a fundamental mechanism in MT and is critical to the automation of MT. Prior studies have proposed approaches for predicting MRs (PMR). One major idea behind these PMR approaches is to represent program source code information via manually designed code features and then to apply machine-learning–based classifiers to automatically predict whether a specific MR can be applied on the target program. Nevertheless, the human-involved procedure of selecting and extracting code features is costly, and it may not be easy to obtain sufficiently comprehensive features for representing source code. To overcome this limitation, in this study, we explore and evaluate the effectiveness of code representation learning techniques for PMR. By applying neural code representation models for automatically mapping program source code to code vectors, the PMR procedure can be boosted with learned code representations. We develop 32 PMR instances by, respectively, combining 8 code representation models with 4 typical classification models and conduct an extensive empirical study to investigate the effectiveness of code representation learning techniques in the context of MR prediction. Our findings reveal that code representation learning can positively contribute to the prediction of MRs and provide insights into the practical usage of code representation models in the context of MR prediction. Our findings could help researchers and practitioners to gain a deeper understanding of the strength of code representation learning for PMR and, hence, pave the way for future research in deriving or extracting MRs from program source code.},
year = {2024}
}
@article{https://doi.org/10.1002/smr.2620,
author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Ge, Jidong and Huang, Liguo and Luo, Bin},
title = {PassSum: Leveraging paths of abstract syntax trees and self-supervision for code summarization},
journal = {Journal of Software: Evolution and Process},
volume = {36},
number = {6},
pages = {e2620},
keywords = {abstract syntax tree, code summarization, self-supervised pretraining, Transformer},
doi = {https://doi.org/10.1002/smr.2620},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2620},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2620},
abstract = {Abstract Code summarization is to provide a high-level comment for a code snippet that typically describes the function and intent of the given code. Recent years have seen the successful application of data-driven code summarization. To improve the performance of the model, numerous approaches use abstract syntax trees (ASTs) to represent the structural information of the code, which is considered by most researchers to be the main factor that distinguishes code from natural language. Then, such data-driven methods are trained on large-scale labeled datasets to obtain a model with strong generalization capabilities that can be applied to new examples. Nevertheless, we argue that state-of-the-art approaches suffer from two key weaknesses: (1) inefficient encoding of ASTs; (2) reliance on a large labeled corpus for model training. As a result, such drawbacks lead to (1) oversized model, slow training, information loss and instability; (2) inability to be applied to programming languages with only a small amount of labeled data. In light of these weaknesses, we propose PassSum, a code summarization approach that addresses the aforementioned weaknesses via (1) a novel input representation which contains an efficient AST encoding method; (2) introducing three pretraining objectives and pretraining our model with a large amount of (easy-to-obtain) unlabeled data under the guidance of self-supervised learning. Experimental results on code summarization for Java, Python, and Ruby methods demonstrate the superiority of PassSum to state-of-the-art methods. Further experiments demonstrate that the input representation we use has both temporal and spatial advantages in addition to performance leadership. In addition, pretraining is also shown to make the model more generalizable with less labeled data, and also to speed up the convergence of the model during training.},
year = {2024}
}
@article{https://doi.org/10.1111/add.15739,
author = {Rogeberg, Ole and Bergsvik, Daniel and Clausen, Thomas},
title = {Opioid overdose deaths and the expansion of opioid agonist treatment: a population-based prospective cohort study},
journal = {Addiction},
volume = {117},
number = {5},
pages = {1363-1371},
keywords = {Drug overdoses, mortality, opioid-related fatalities, opioid agonist treatment, poisson difference-in-differences mod, populations-based cohort study},
doi = {https://doi.org/10.1111/add.15739},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/add.15739},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/add.15739},
abstract = {Abstract Background and aim Effective policies to reduce drug-related overdoses remain a public health priority. We aimed to estimate the causal effects of a national opioid agonist treatment (OAT) program on population level drug fatalities. Design Population-based prospective cohort study exploiting supply driven variation in treatment uptake across cohort-age groups generated by the introduction and scale-up of a national OAT program. A Poisson difference-in-differences model with an intention-to-treat design was used to assess how treatment uptake altered the age profile of risks and infer treatment effects on drug fatalities. Setting Norway, from 1996 through 2016. Cases The data include a total of 5634 drug-related overdose deaths and cover the introduction of the Norwegian OAT program in 1998 and its initial growth period, reaching 12 286 ever-treated recipients by 2016. Measurements Fatal opioid-related overdoses were defined as deaths with a primary cause assigned an International Classification of Diseases 10th Revision (ICD-10) code F11, or X42, X44, X62 or X64 in combination with T40.0–T40.4. Other non-opioid related fatal overdoses were defined by a primary cause registered as F12, F14, F15, F16 or F19, or X42, X44, X62 or 64 in combination with T40.5–T40.9. Findings An additional 887 deaths (95\% credibility interval [CI] = 265–1563) would have been expected in the absence of OAT, which implies one death avoided per 111 (95\% CI = 61–342) treatment-exposed person-years. At scale, the program reduced annual overdose mortality by 27\% in 2016 (95\% CI = 10\%–41\%) relative to a no-OAT counterfactual, corresponding to 99 fewer expected fatal overdoses (95\% CI = 28–180) in 2016. Analysing fatal opioid-related and other drug overdoses separately found similar numbers for avoided opioid-related fatalities (921, with 95\% CI = 373–1526) and no treatment effects on non-opioid related fatalities (−38, with 95\% CI = −193–154). Conclusion The introduction and rapid scale-up of a national opioid agonist treatment program in Norway was associated with substantial and plausibly causal reductions in drug fatalities.},
year = {2022}
}
@article{https://doi.org/10.1111/jzo.13176,
author = {Iwai, N.},
title = {Effects of growing season and individual growth rates on the occurrence of larval overwintering in Otton frog tadpoles},
journal = {Journal of Zoology},
volume = {323},
number = {4},
pages = {284-291},
keywords = {amphibian, Babina subaspera, metamorphosis, rearing experiment, survival, dormancy, tadpoles},
doi = {https://doi.org/10.1111/jzo.13176},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jzo.13176},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jzo.13176},
note = { JZO-07-23-ORR-200.R2},
abstract = {Abstract The timing and size of metamorphosis are crucial for the future fitness of organisms with complex life cycles, such as amphibians. In some amphibian species, a portion of tadpoles will metamorphose in their natal year whereas others from the same cohort overwinter as tadpoles. Low temperature and food availability were previously assumed to be the underlying reasons; however, the factors influencing the decision to overwinter as tadpoles or metamorphose in the natal year have not been extensively studied. This study investigated these factors by conducting laboratory-rearing experiments on Babina subaspera tadpoles. The tadpoles were individually reared under controlled temperature and light conditions that simulated five different growing seasons (i.e., the start month) observed in the field. Four different food quantity levels were set to induce different growth rates, which were measured individually. The results revealed that 33 tadpoles metamorphosed within their natal year, whereas 52 tadpoles were overwintered as tadpoles and metamorphosed the following spring. The size at metamorphosis was larger in tadpoles that metamorphosed after winter than in those that metamorphosed before winter. Whether tadpoles metamorphosed before or after winter was influenced by the individual growth rate and growing season. Tadpoles with slower growth rates were more likely to metamorphose after winter, possibly because slower growth prevented them from reaching the critical size threshold required for metamorphosis before winter. The threshold for the occurrence of larval overwintering varied with the growing season; tadpoles that spawned later in the year were more likely to overwinter, even with a high growth rate. The results suggested that slow-growing B. subaspera tadpoles with insufficient time until the onset of winter would gain a higher fitness by metamorphosing after winter. This would be advantageous due to the tadpoles' potentially high survival rate during dormancy as well as their larger size at metamorphosis in the following year.},
year = {2024}
}
@article{https://doi.org/10.1002/cpt.3203,
author = {Janssen, Alexander and Smalbil, Louk and Bennis, Frank C. and Cnossen, Marjon H. and Mathôt, Ron A. A. and { the OPTI-CLOT study group and SYMPHONY consortium }},
title = {A Generative and Causal Pharmacokinetic Model for Factor VIII in Hemophilia A: A Machine Learning Framework for Continuous Model Refinement},
journal = {Clinical Pharmacology \& Therapeutics},
volume = {115},
number = {4},
pages = {881-889},
doi = {https://doi.org/10.1002/cpt.3203},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpt.3203},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpt.3203},
note = { 2023-0818},
abstract = {In rare diseases, such as hemophilia A, the development of accurate population pharmacokinetic (PK) models is often hindered by the limited availability of data. Most PK models are specific to a single recombinant factor VIII (rFVIII) concentrate or measurement assay, and are generally unsuited for answering counterfactual (“what-if”) queries. Ideally, data from multiple hemophilia treatment centers are combined but this is generally difficult as patient data are kept private. In this work, we utilize causal inference techniques to produce a hybrid machine learning (ML) PK model that corrects for differences between rFVIII concentrates and measurement assays. Next, we augment this model with a generative model that can simulate realistic virtual patients as well as impute missing data. This model can be shared instead of actual patient data, resolving privacy issues. The hybrid ML-PK model was trained on chromogenic assay data of lonoctocog alfa and predictive performance was then evaluated on an external data set of patients who received octocog alfa with FVIII levels measured using the one-stage assay. The model presented higher accuracy compared with three previous PK models developed on data similar to the external data set (root mean squared error = 14.6 IU/dL vs. mean of 17.7 IU/dL). Finally, we show that the generative model can be used to accurately impute missing data (< 18\% error). In conclusion, the proposed approach introduces interesting new possibilities for model development. In the context of rare disease, the introduction of generative models facilitates sharing of synthetic data, enabling the iterative improvement of population PK models.},
year = {2024}
}
@article{https://doi.org/10.1111/pirs.12524,
author = {Lewin, Paul A. and Weber, Bruce A.},
title = {Distributional impacts of food assistance: How SNAP payments to the rural poor affect incomes in the urban core},
journal = {Papers in Regional Science},
volume = {99},
number = {5},
pages = {1281-1300},
keywords = {central place hierarchy, core-Periphery interaction, food security policy, food stamp, multi-regional SAM model, policy spillover, Portland trade area, SNAP, Supplemental Nutrition Assistance Program},
doi = {https://doi.org/10.1111/pirs.12524},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pirs.12524},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pirs.12524},
note = { PIRS-FA-2019-2560.R2},
abstract = {Abstract This paper examines how benefits of the Supplemental Nutrition Assistance Program (SNAP) to low-income households spillover between Portland, Oregon-metropolitan core and its periphery trade area. The paper studies the impact of the SNAP program on four income classes by comparing the income increases generated by SNAP in 2006 in each income class in each region with a counterfactual scenario in which the taxes that would be needed from taxpayers in the region to support SNAP are treated as income and spent. The analysis shows that overall, SNAP payments net of taxes stimulate the regional economy and are significantly redistributive.},
year = {2020}
}
@article{https://doi.org/10.1049/cit2.12207,
author = {Yang, Yilong and Liu, Yibo and Bao, Tianshu and Wang, Weiru and Niu, Nan and Yin, Yongfeng},
title = {DeepOCL: A deep neural network for Object Constraint Language generation from unrestricted nature language},
journal = {CAAI Transactions on Intelligence Technology},
volume = {9},
number = {1},
pages = {250-263},
keywords = {deep learning, OCL, software engineering},
doi = {https://doi.org/10.1049/cit2.12207},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cit2.12207},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/cit2.12207},
abstract = {Abstract Object Constraint Language (OCL) is one kind of lightweight formal specification, which is widely used for software verification and validation in NASA and Object Management Group projects. Although OCL provides a simple expressive syntax, it is hard for the developers to write correctly due to lacking knowledge of the mathematical foundations of the first-order logic, which is approximately half accurate at the first stage of development. A deep neural network named DeepOCL is proposed, which takes the unrestricted natural language as inputs and automatically outputs the best-scored OCL candidates without requiring a domain conceptual model that is compulsively required in existing rule-based generation approaches. To demonstrate the validity of our proposed approach, ablation experiments were conducted on a new sentence-aligned dataset named OCLPairs. The experiments show that the proposed DeepOCL can achieve state of the art for OCL statement generation, scored 74.30 on BLEU, and greatly outperformed experienced developers by 35.19\%. The proposed approach is the first deep learning approach to generate the OCL expression from the natural language. It can be further developed as a CASE tool for the software industry.},
year = {2024}
}
@article{https://doi.org/10.1002/smr.2586,
author = {Ge, Hongliang and Zhong, Wenkang and Li, Chuanyi and Ge, Jidong and Hu, Hao and Luo, Bin},
title = {RobustNPR: Evaluating the robustness of neural program repair models},
journal = {Journal of Software: Evolution and Process},
volume = {36},
number = {4},
pages = {e2586},
keywords = {model evaluation, neural program repair, robustness},
doi = {https://doi.org/10.1002/smr.2586},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2586},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2586},
abstract = {Abstract Due to the high cost of repairing defective programs, many researches focus on automatic program repair (APR). In recent years, the new trend of APR is to apply neural networks to mine the relations between defective programs and corresponding patches automatically, which is known as neural program repair (NPR). The community, however, ignores some important properties that could impact the applicability of NPR systems, such as robustness. For semantic-identical buggy programs, NPR systems may produce totally different patches. In this paper, we propose an evaluation tool named RobustNPR, the first NPR robustness evaluation tool. RobustNPR employs several mutators to generate semantic-identical mutants of defective programs. For an original defective program and its mutant, it checks two aspects of NPR: (a) Can NPR fix mutants when it can fix the original defective program? and (b) can NPR generate semantic-identical patches for the original program and the mutant? Then, we evaluate four SOTA NPR models and analyze the results. From the results, we find that even for the best-performing model, 20.16\% of the repair success is unreliable, which indicates that the robustness of NPR is not perfect. In addition, we find that the robustness of NPR is correlated with model settings and other factors.},
year = {2024}
}
@article{https://doi.org/10.1111/nous.12385,
author = {Munroe, Wade},
title = {Why are you talking to yourself? The epistemic role of inner speech in reasoning},
journal = {Noûs},
volume = {56},
number = {4},
pages = {841-866},
doi = {https://doi.org/10.1111/nous.12385},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nous.12385},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/nous.12385},
abstract = {Abstract People frequently report that, at times, their thought has a vocal character. Thinking commonly appears to be accompanied or constituted by silently ‘talking’ to oneself in inner speech. In this paper, we explore the specifically epistemic role of inner speech in conscious reasoning. A plausible position—but one I argue is ultimately wrong—is that inner speech plays a solely facilitative role that is exhausted by (i) serving as the vehicle of representation for conscious reasoning, and/or (ii) allowing one to focus on certain types of objects or relations, e.g., causal relations, abstracta, counterfactuals, etc., or to consciously entertain structured propositional contents that it would be hard (or impossible) to focus on or entertain with representations in other (e.g., imagistic) formats. According to this position, inner speech doesn't figure as a justificatory element in our reasoning or as the partial epistemic basis of our conclusions—it merely facilitates reasoning through (i) and/or (ii). In contrast to the view that inner speech is a mere facilitator, I establish that (outside of potentially playing roles (i) and/or (ii)) the language we use itself serves as a crucial source of information in reasoning. In other words, we reason from propositions about the language we use in inner speech as opposed to exclusively reasoning from the semantic contents of the speech. My conclusion follows from how we use language as a cognitive tool to keep track of information, e.g., the contents of premises, lemmas, previous reasoning results, etc., in reasoning.},
year = {2022}
}
@article{https://doi.org/10.1002/sys.21710,
author = {Mitola III, Joseph and Prys, Mark},
title = {Cyber oriented digital engineering},
journal = {Systems Engineering},
volume = {27},
number = {1},
pages = {109-119},
keywords = {chaos monkey, critical infrastructure, cybersecurity, digital engineering, hardware cyber hardening, systems engineering, zero trust},
doi = {https://doi.org/10.1002/sys.21710},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sys.21710},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sys.21710},
abstract = {Abstract The purpose of cyber oriented digital engineering (CODE) is to provide a repeatable systems-engineering process for systems to resist Stuxnet-class advanced persistent threat (APT) cyberattacks. CODE integrates cyber thinking with systems thinking. CODE accomplishes this by extending the US Department of Defense (DoD) digital engineering (DE) framework with functional mission analysis for cyber (FMA-C), hybrid cloud architecture, zero trust (ZT) principles, threat analysis, and hardware cyber hardening (HCH). The lack of success of red team attacks conducted in our laboratory against an exemplar system demonstrates how following the CODE systems engineering process actually does “bake cybersecurity into the system”, making the resulting systems architecture and implementations more resilient. In a recent Pilot Project, CODE enhanced the systems requirements document of the top left side of the systems engineering Vee. CODE Pilot requirements embodied ZT principles, including machine to machine (M2M) credential exchanges and internal self-checking. We anticipate working with International Council on Systems Engineering (INCOSE), the object management group (OMG,) and others towards standardizing CODE's cyber-systems engineering processes for broader use of the global systems engineering community.},
year = {2024}
}
@article{https://doi.org/10.1002/stvr.1845,
author = {Fontes, Afonso and Gay, Gregory},
title = {The integration of machine learning into automated test generation: A systematic mapping study},
journal = {Software Testing, Verification and Reliability},
volume = {33},
number = {4},
pages = {e1845},
keywords = {automated test generation, machine learning, test case generation, test input generation, test oracle generation},
doi = {https://doi.org/10.1002/stvr.1845},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1845},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1845},
abstract = {Abstract Machine learning (ML) may enable effective automated test generation. We characterize emerging research, examining testing practices, researcher goals, ML techniques applied, evaluation, and challenges in this intersection by performing. We perform a systematic mapping study on a sample of 124 publications. ML generates input for system, GUI, unit, performance, and combinatorial testing or improves the performance of existing generation methods. ML is also used to generate test verdicts, property-based, and expected output oracles. Supervised learning—often based on neural networks—and reinforcement learning—often based on Q-learning—are common, and some publications also employ unsupervised or semi-supervised learning. (Semi-/Un-)Supervised approaches are evaluated using both traditional testing metrics and ML-related metrics (e.g., accuracy), while reinforcement learning is often evaluated using testing metrics tied to the reward function. The work-to-date shows great promise, but there are open challenges regarding training data, retraining, scalability, evaluation complexity, ML algorithms employed—and how they are applied—benchmarks, and replicability. Our findings can serve as a roadmap and inspiration for researchers in this field.},
year = {2023}
}
@article{https://doi.org/10.1029/2021GL096898,
author = {Huang, Cheng and Bai, Cong and Chan, Sixian and Zhang, Jinglin},
title = {MMSTN: A Multi-Modal Spatial-Temporal Network for Tropical Cyclone Short-Term Prediction},
journal = {Geophysical Research Letters},
volume = {49},
number = {4},
pages = {e2021GL096898},
doi = {https://doi.org/10.1029/2021GL096898},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021GL096898},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021GL096898},
note = {e2021GL096898 2021GL096898},
abstract = {Abstract Forecasting the trajectory and intensity of tropical cyclones (TCs) is important in disaster mitigation as TC usually causes huge damages. However, it remains a substantial challenge due to the limited understanding of TC complexity. Still, TCs have been observed and recorded for several decades, and so can be predicted if viewed as a spatial-temporal prediction problem with a huge amount of existing data. We propose a novel TC trajectory and intensity short-term prediction method: Multi-Modal Spatial-temporal Networks (MMSTN). It not only predicts the TC's central pressure, winds, and the location of its center, but also forecasts the TC's varied possible tendencies. Experiments were conducted on the China Meteorological Administration Tropical Cyclone Best Track Dataset. Experimental results show that the proposed MMSTN outperformed state-of-the-art methods as well as the official prediction method of the China Central Meteorological Observatory, in intensity prediction and 6 hr trajectory prediction.},
year = {2022}
}
@article{https://doi.org/10.1155/2021/5533963,
author = {Pan, Zulie and Chen, Yuanchao and Chen, Yu and Shen, Yi and Guo, Xuanzhen},
title = {Webshell Detection Based on Executable Data Characteristics of PHP Code},
journal = {Wireless Communications and Mobile Computing},
volume = {2021},
number = {1},
pages = {5533963},
doi = {https://doi.org/10.1155/2021/5533963},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/5533963},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/5533963},
abstract = {A webshell is a malicious backdoor that allows remote access and control to a web server by executing arbitrary commands. The wide use of obfuscation and encryption technologies has greatly increased the difficulty of webshell detection. To this end, we propose a novel webshell detection model leveraging the grammatical features extracted from the PHP code. The key idea is to combine the executable data characteristics of the PHP code with static text features for webshell classification. To verify the proposed model, we construct a cleaned data set of webshell consisting of 2,917 samples from 17 webshell collection projects and conduct extensive experiments. We have designed three sets of controlled experiments, the results of which show that the accuracy of the three algorithms has reached more than 99.40\%, the highest reached 99.66\%, the recall rate has been increased by at least 1.8\%, the most increased by 6.75\%, and the F1 value has increased by 2.02\% on average. It not only confirms the efficiency of the grammatical features in webshell detection but also shows that our system significantly outperforms several state-of-the-art rivals in terms of detection accuracy and recall rate.},
year = {2021}
}
@article{https://doi.org/10.1111/maps.13542,
author = {Grieve, Richard A. F. and Osinski, Gordon R.},
title = {The Upper Contact Unit of the Sudbury Igneous Complex in the Garson region: Constraints on the depth of origin of a peak ring at the Sudbury impact structure},
journal = {Meteoritics \& Planetary Science},
volume = {55},
number = {8},
pages = {},
doi = {https://doi.org/10.1111/maps.13542},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/maps.13542},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/maps.13542},
abstract = {Abstract Observational and logical arguments are presented for the lithology formerly named the Garson Member of the Onaping Formation being the clast-bearing, fine-grained, chilled Upper Contact Unit (UCU) of the Sudbury Igneous Complex (SIC) in the Garson region of the Sudbury impact structure. It differs considerably, however, from the UCU in the North Range of the SIC with respect to the character of its clasts. Namely, the clasts are essentially monomict (quartzites), much larger (up to 100 m across), and much more abundant (up to 80\% in places). These differences indicate a different source than “fallback” material for the clasts in the UCU in the Garson region. Their character requires a “coherent,” singular source that was topographically above the SIC melt pool. Such a source would correspond to that of an emergent peak ring of fractured target rocks. The clasts are identified as Huronian Mississagi quartzite, which is estimated to have been at a nominal depth of 7.5 ± 2.5 km at the time of impact. This provides a constraint on the depth of origin of the peak ring. This depth estimate is closest to the lower depth estimate from current numerical models of Sudbury and the similar-sized Chicxulub impact structures.},
year = {2020}
}
@article{https://doi.org/10.1002/aps3.11331,
author = {Ledesma, Dakila A. and Powell, Caleb A. and Shaw, Joey and Qin, Hong},
title = {Enabling automated herbarium sheet image post-processing using neural network models for color reference chart detection},
journal = {Applications in Plant Sciences},
volume = {8},
number = {3},
pages = {e11331},
keywords = {automation, digitization, herbarium, machine learning, natural history collections, specimen images},
doi = {https://doi.org/10.1002/aps3.11331},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aps3.11331},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aps3.11331},
abstract = {Premise Large-scale efforts to digitize herbaria have resulted in more than 18 million publicly available Plantae images on sites such as iDigBio. The automation of image post-processing will lead to time savings in the digitization of biological specimens, as well as improvements in data quality. Here, new and modified neural network methodologies were developed to automatically detect color reference charts (CRC), enabling the future automation of various post-processing tasks. Methods and Results We used 1000 herbarium specimen images from 52 herbaria to test our novel neural network model, ColorNet, which was developed to identify CRCs smaller than 4 cm2, resulting in a 30\% increase in accuracy over the performance of other state-of-the-art models such as Faster R-CNN. For larger CRCs, we propose modifications to Faster R-CNN to increase inference speed. Conclusions Our proposed neural networks detect a range of CRCs, which may enable the automation of post-processing tasks found in herbarium digitization workflows, such as image orientation or white balance correction.},
year = {2020}
}
@article{https://doi.org/10.1155/2021/5538841,
author = {Feng, Pengbin and Ma, Jianfeng and Li, Teng and Ma, Xindi and Xi, Ning and Lu, Di},
title = {Android Malware Detection via Graph Representation Learning},
journal = {Mobile Information Systems},
volume = {2021},
number = {1},
pages = {5538841},
doi = {https://doi.org/10.1155/2021/5538841},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/5538841},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/5538841},
abstract = {With the widespread usage of Android smartphones in our daily lives, the Android platform has become an attractive target for malware authors. There is an urgent need for developing an automatic malware detection approach to prevent the spread of malware. The low code coverage and poor efficiency of the dynamic analysis limit the large-scale deployment of malware detection methods based on dynamic features. Therefore, researchers have proposed a plethora of detection approaches based on abundant static features to provide efficient malware detection. This paper explores the direction of Android malware detection based on graph representation learning. Without complex feature graph construction, we propose a new Android malware detection approach based on lightweight static analysis via the graph neural network (GNN). Instead of directly extracting Application Programming Interface (API) call information, we further analyze the source code of Android applications to extract high-level semantic information, which increases the barrier of evading detection. Particularly, we construct approximate call graphs from function invocation relationships within an Android application to represent this application and further extract intrafunction attributes, including required permission, security level, and Smali instructions’ semantic information via Word2Vec, to form the node attributes within graph structures. Then, we use the graph neural network to generate a vector representation of the application, and then malware detection is performed on this representation space. We conduct experiments on real-world application samples. The experimental results demonstrate that our approach implements high effective malware detection and outperforms state-of-the-art detection approaches.},
year = {2021}
}
@article{https://doi.org/10.1155/2022/1564178,
author = {Xu, Aiqiao},
title = {[Retracted] Software Engineering Code Workshop Based on B-RRT∗FND Algorithm for Deep Program Understanding Perspective},
journal = {Journal of Sensors},
volume = {2022},
number = {1},
pages = {1564178},
doi = {https://doi.org/10.1155/2022/1564178},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/1564178},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/1564178},
abstract = {Developers will perform a lot of search behaviors when facing daily work tasks, searching for reusable code fragments, solutions to specific problems, algorithm designs, software documentation, and software tools from public repositories (including open source communities and forum blogs) or private repositories (internal software repositories, source code platforms, communities, etc.) to make full use of existing software development resources and experiences. This paper first takes a deep programmatic understanding view of the software development process. In this paper, we first define the software engineering code search task from the perspective of deep program understanding. Secondly, this paper summarizes two research paradigms of deep software engineering code search and composes the related research results. At the same time, this paper summarizes and organizes the common evaluation methods for software engineering code search tasks. Finally, the results of this paper are combined with an outlook on future research.},
year = {2022}
}
@article{https://doi.org/10.1002/hec.4199,
author = {Feyman, Yevgeniy and Pizer, Steven D. and Frakt, Austin B.},
title = {The persistence of medicare advantage spillovers in the post-Affordable Care Act era},
journal = {Health Economics},
volume = {30},
number = {2},
pages = {311-327},
keywords = {managed care, Medicare Advantage, spillovers},
doi = {https://doi.org/10.1002/hec.4199},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hec.4199},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/hec.4199},
abstract = {Abstract Spillovers can arise in markets with multiple purchasers relying on shared producers. Prior studies have found such spillovers in health care, from managed care to nonmanaged care populations—reducing spending and utilization, and improving outcomes, including in Medicare. This study provides the first plausibly causal estimates of such spillovers from Medicare Advantage (MA) to Traditional Medicare (TM) in the post-Affordable Care Act era using an instrumental variables approach. Controlling for health status and other potential confounders, we estimate that a one percentage point increase in county-level MA penetration results in a \$64 (95\% CI: \$18 to \$110) (0.7\%) reduction in standardized per-enrollee TM spending. We find evidence for reductions in utilization both on the intensive and extensive margins, across a number of health care services. Our results complement and extend prior work that found spillovers from MA to TM in earlier years and under different payment policies than are in place today.},
year = {2021}
}
@article{https://doi.org/10.1111/pech.12523,
author = {Arai, Tatsushi},
title = {Functional Coexistence in Intractable Conflict: A Decades-long View of Conflict Intervention},
journal = {Peace \& Change},
volume = {47},
number = {2},
pages = {118-151},
doi = {https://doi.org/10.1111/pech.12523},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pech.12523},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pech.12523},
abstract = {Abstract This article develops a theory of functional coexistence, a sustained negative peace which enables conflict parties and intermediaries to resist premature settlement and stay constructively engaged in an enduring state of nonresolution. The case studies of China-Taiwan relations and the divided Europe and Germany during the Cold War illustrate essential qualities of functional coexistence. These qualities include the dynamic nature of the perceived boundaries of controlled interactions between adversaries who deny their opponents’ legitimacy and, in extreme cases, even their right to exist. Key lessons from the case studies are: (1) the need to build historical awareness of the shifting boundaries of mutually acceptable conflict behavior, (2) the imperative of finding a constructive way of staying engaged in intractable conflict, (3) the role of a multi-layered, structural view of conflict intervention capable of resisting the inertia of hegemonic stability and submissive inaction, and (4) the importance of making use of short-term actions to facilitate long-term social change.},
year = {2022}
}
@article{https://doi.org/10.1002/jac5.1856,
author = {Wong, Adrian and Wentz, Erin and Palisano, Nicholas and Dirani, Manar and Elsamadisi, Pansy and Qashou, Farah and Celi, Leo and Badawi, Omar and Nazer, Lama},
title = {Role of artificial intelligence in pharmacy practice: A narrative review},
journal = {JACCP:  JOURNAL OF THE AMERICAN COLLEGE OF CLINICAL PHARMACY},
volume = {6},
number = {11},
pages = {1237-1250},
keywords = {artificial intelligence, patient safety, pharmacy, precision medicine, review},
doi = {https://doi.org/10.1002/jac5.1856},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jac5.1856},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jac5.1856},
abstract = {Abstract Artificial intelligence (AI) has emerged as a potentially useful tool in transforming healthcare. Roles for AI include drug safety, operations, as well as precision medicine to improve patient outcomes. Within pharmacy practice, limited guidance is available describing the purpose of AI. This gap is important to address given the current growth of AI in medicine. The purpose of this narrative review was to review the literature evaluating the potential role of AI in three relevant areas of pharmacy practice. A review of Medline and Embase from January 2004 to June 2022 evaluating search terms involving AI and drug safety, pharmacy operations, or precision medicine. Articles had to be focused on human subjects, be either an observational cohort study or randomized control trial, and involve medication use to be included in this review. Data collection included the study design, AI methodology, and primary outcome and associated AI model performance measures. A total of 24 232 articles were identified from the search strategy, with 403 articles meeting inclusion criteria (1.7\%). A total of 22 articles were included in this review. Articles focused on drug safety included safe use of opioids and the role of social media in identifying adverse drug reactions. In terms of pharmacy operations, articles included the identification of medications at risk for shortage and visual identification of medications. Finally, for precision medicine, articles included determining likelihood of beneficial effects of medications for new diagnoses and dosing of warfarin. The role of AI in improving patient outcomes is numerous. However, challenges that remain include transparency of generated AI models, generalizability, and lack of external validation. Future areas of focus include education of pharmacy clinicians on the role of AI and the increased use of open-source models/data to allow for external validation of AI.},
year = {2023}
}
@article{https://doi.org/10.1049/cvi2.12291,
author = {Zhu, Peng-Jie and Pu, Yuan-Yuan and Yang, Qiuxia and Li, Siqi and Zhao, Zheng-Peng and Wu, Hao and Xu, Dan},
title = {PSANet: Automatic colourisation using position-spatial attention for natural images},
journal = {IET Computer Vision},
volume = {18},
number = {7},
pages = {922-934},
keywords = {convolutional neural nets, image processing},
doi = {https://doi.org/10.1049/cvi2.12291},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/cvi2.12291},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1049/cvi2.12291},
abstract = {Abstract Due to the richness of natural image semantics, natural image colourisation is a challenging problem. Existing methods often suffer from semantic confusion due to insufficient semantic understanding, resulting in unreasonable colour assignments, especially at the edges of objects. This phenomenon is referred to as colour bleeding. The authors have found that using the self-attention mechanism benefits the model's understanding and recognition of object semantics. However, this leads to another problem in colourisation, namely dull colour. With this in mind, a Position-Spatial Attention Network(PSANet) is proposed to address the colour bleeding and the dull colour. Firstly, a novel new attention module called position-spatial attention module (PSAM) is introduced. Through the proposed PSAM module, the model enhances the semantic understanding of images while solving the dull colour problem caused by self-attention. Then, in order to further prevent colour bleeding on object boundaries, a gradient-aware loss is proposed. Lastly, the colour bleeding phenomenon is further improved by the combined effect of gradient-aware loss and edge-aware loss. Experimental results show that this method can reduce colour bleeding largely while maintaining good perceptual quality.},
year = {2024}
}
@article{https://doi.org/10.1111/1468-2230.12461,
author = {Dowds, Eithne},
title = {Towards a Contextual Definition of Rape: Consent, Coercion and Constructive Force},
journal = {The Modern Law Review},
volume = {83},
number = {1},
pages = {35-63},
doi = {https://doi.org/10.1111/1468-2230.12461},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-2230.12461},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-2230.12461},
abstract = {Abstract This paper considers ‘consent-based’ and ‘coercion-based’ models of defining rape. It argues that the ability of these models to adequately protect against violations of sexual autonomy is dependent on their engagement with the broader circumstances within which sexual choices are made. Following an analysis of both models it is argued that attempts to contextualise consent and coercion are often undermined by evaluative framings that encourage scrutiny of the complainant's actions at the expense of engagement with the broader circumstances. This is particularly problematic where rape occurs as a result of non-violent coercion and the victim does not verbally or physically demonstrate their lack of consent. The paper draws on United States military law and argues that the doctrine of constructive force, which has been used to deal with non-violent coercion in these contexts, has the potential to progressively reshape our contextual and evaluative framings in domestic contexts.},
year = {2020}
}
@article{https://doi.org/10.1002/fer3.10,
author = {Liu, Ming and Ren, Yiling and Nyagoga, Lucy Michael and Stonier, Francis and Wu, Zhongming and Yu, Liang},
title = {Future of education in the era of generative artificial intelligence: Consensus among Chinese scholars on applications of ChatGPT in schools},
journal = {Future in Educational Research},
volume = {1},
number = {1},
pages = {72-101},
keywords = {ChatGPT, Chinese scholar, digital transformation of education, generative artificial intelligence},
doi = {https://doi.org/10.1002/fer3.10},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fer3.10},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/fer3.10},
abstract = {Abstract ChatGPT is an artificial intelligence chatbot that utilizes advanced natural language processing technologies, including large language models, to produce human-like responses to user queries spanning a wide range of topics from programming to mathematics. As an emerging generative artificial intelligence (GAI) tool, it presents novel opportunities and challenges to the ongoing digital transformation of education. This article employs a systematic review approach to summarize the viewpoints of Chinese scholars and experts regarding the implementation of GAI in education. The research findings indicate that a majority of Chinese scholars support the cautious integration of GAI into education as it serves as a learning tool that offers personalized educational experiences for students. However, it also raises concerns related to academic integrity and the potential hindrance to students' critical thinking skills. Consequently, a framework called DATS, which outlines an optimization path for future GAI applications in schools, is proposed. The framework takes into account the perspectives of four key stakeholders: developers, administrators, teachers, and students.},
year = {2023}
}
@article{https://doi.org/10.1111/pirs.12430,
author = {Martin, Ron and Gardiner, Ben},
title = {The resilience of cities to economic shocks: A tale of four recessions (and the challenge of Brexit)},
journal = {Papers in Regional Science},
volume = {98},
number = {4},
pages = {1801-1832},
keywords = {Brexit, cities, recessionary shocks, recoverability, resilience, resistance},
doi = {https://doi.org/10.1111/pirs.12430},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pirs.12430},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pirs.12430},
abstract = {Abstract This paper examines the resilience of British cities to major economic shocks. Using a novel data set for 85 cities, it analyses their resistance to and recovery from the last four major recessions, over the period 1971 to 2015. It reveals a distinct shift in the relation between resistance and recovery between these shocks, as well as major differences between northern and southern cities. Some possible factors shaping these patterns are explored, and tentative estimates of the likely impact of the Brexit shock (Britain's withdrawal from the European Union) are also provided. A key implication is that differences in resilience to major shocks can contribute to the long-run growth paths of cities.},
year = {2019}
}
@article{https://doi.org/10.1002/stvr.1900,
author = {Tamagnan, Frédéric and Vernotte, Alexandre and Bouquet, Fabrice and Legeard, Bruno},
title = {Generation of Regression Tests From Logs With Clustering Guided by Usage Patterns},
journal = {Software Testing, Verification and Reliability},
volume = {34},
number = {8},
pages = {e1900},
keywords = {clustering, pattern mining, regression test selection, user traces},
doi = {https://doi.org/10.1002/stvr.1900},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1900},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1900},
note = {e1900 stvr.1900},
abstract = {ABSTRACT Clustering is increasingly being used to select the appropriate test suites. In this paper, we apply this approach to regression testing. Regression testing is the practice of verifying the robustness and reliability of software by retesting after changes have been made. Creating and maintaining functional regression tests is a laborious and costly activity. To be effective, these tests must represent the actual user journeys of the application. In addition, an optimal number of test cases is critical for the rapid execution of the regression test suite to stay within the time and computational resource budget as it is re-run at each major iteration of the software development. Therefore, the selection and maintenance of functional regression tests based on the analysis of application logs has gained popularity in recent years. This paper presents a novel approach to improve regression testing by automating the creation of test suites using user traces fed into clustering pipelines. Our methodology introduces a new metric based on pattern mining to quantify the statistical coverage of prevalent user paths. This metric helps to determine the optimal number of clusters within a clustering pipeline, thus addressing the challenge of suboptimal test suite sizes. Additionally, we introduce two criteria, to systematically evaluate and rank clustering pipelines. Experimentation involving 33 variations of clustering pipelines across four datasets demonstrates the potential effectiveness of our automated approach compared with manually crafted test suites. (All the experiments and data on Scanner, Spree and Booked Scheduler are available at https://github.com/frederictamagnan/STVR2024.) Then, we analyse the semantics of the clusters based on their principal composing patterns.},
year = {2024}
}
@article{https://doi.org/10.1111/psyp.14255,
author = {Wood, Matthew and Grenier, Amandine E. and Wicha, Nicole Y. Y.},
title = {Development is in the details: Event-related theta oscillations reveal children and adults verify multiplication facts differently},
journal = {Psychophysiology},
volume = {60},
number = {6},
pages = {e14255},
keywords = {arithmetic, children, event related potentials, math cognition, multiplication verification, N400, P300, theta power, time-frequency analysis, word-picture verification},
doi = {https://doi.org/10.1111/psyp.14255},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.14255},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.14255},
note = {e14255 PsyP-2021-0594.R2},
abstract = {Abstract When verifying the correctness of single-digit multiplication problems, children and adults show a robust ERP correctness effect thought to reflect similar cognitive processes across groups. Recent studies suggest that this effect is instead a modulation of the negative-going N400 component in children, reflecting access to semantic memory, and the positive-going P300 component in adults, reflecting stimulus categorization. However, the relative difference in ERP amplitude is the same for both components, more positive for correct than incorrect solutions, presenting a challenge to ascertaining the appropriate interpretation. Time-frequency analysis (TFA) of the N400/P300 window provides an objective approach to dissociating these effects. TFA measured from solution onset during single-digit multiplication verification revealed significant modulations of event-related as theta power (3–6 Hz) in both groups. Correct trials elicit less power in children (9–12 years) and more power in adults relative to incorrect trials. These findings are consistent with modulations of the N400 and P300, respectively, where opposite effects were predicted for spectral power. The ERP results further support a reinterpretation of the multiplication correctness effect. In contrast, TFA of the N400 effect elicited to a word-picture verification task revealed the same event-related theta effect in both groups, with increased power for mismatched than matched pictures. Together, these findings provide evidence for a developmental shift in cognitive processing specific to the multiplication task. Models of arithmetic should account for this overlooked difference in cognitive processing between children and adults.},
year = {2023}
}
@article{https://doi.org/10.1111/pirs.12672,
author = {de Almeida, Edilberto Tiago and da Mota Silveira Neto, Raul and de Moraes Rocha, Roberta},
title = {Manufacturing location patterns in Brazil},
journal = {Papers in Regional Science},
volume = {101},
number = {4},
pages = {839-873},
keywords = {Brazilian manufacturing industries, location patterns, micro-geographic data},
doi = {https://doi.org/10.1111/pirs.12672},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pirs.12672},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pirs.12672},
abstract = {Abstract In this paper, we present a detailed and comprehensive analysis of the location patterns of manufacturing industries in Brazil using a distance-based measure and micro-geographic panel data for a 10-year period. Our results show that 89.9\% and 91\% of manufacturing at the 3-digit level have statistically significant localization for 2006 and 2015, respectively, and that these patterns remain high when we consider 4-digit classification. High-tech industries have location patterns at short distances, being localized mainly in large urban areas, while low-tech industries are also localized at long distances. We also present evidence that agglomeration economies affect the degree of spatial concentration.},
year = {2022}
}
@article{https://doi.org/10.1111/polp.12618,
author = {Moreira Ramalho, Tiago and Massart, Tom and Crespy, Amandine},
title = {Resilient austerity? National economic discourses before the pandemic in the European Union},
journal = {Politics \& Policy},
volume = {52},
number = {5},
pages = {963-991},
keywords = {2014–2020, austerity, EU economic governance, European Union, Eurozone, fiscal discipline, frame resilience, France, Germany, ideas, international comparative analysis, national budget making, national parliaments, pandemic, parliamentary budget debates, politicization, Portugal, pre-COVID-19, the Netherlands, 经济治理, 欧盟, 财政纪律, 思想, 国家议会, 政治化, gobernanza económica, Unión Europea, disciplina fiscal, ideas, parlamentos nacionales, politización},
doi = {https://doi.org/10.1111/polp.12618},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/polp.12618},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/polp.12618},
note = { P\&P-23-2772.R2},
abstract = {Abstract After the euro crisis, politicization patterns led the institutions of the European Union to gradually redirect the bloc's socioeconomic governance away from austerity. It is less clear whether the erosion of austerity was mirrored in national economic discourses. To fill this gap, this article provides a quantitative and qualitative analysis of parliamentary budget debates in four country cases: France, Germany, the Netherlands, and Portugal, from 2014 to 2020. The results show contrasting patterns of “fiscal discipline” frame resilience in national economic discourse in ways consistent with intergovernmental bargaining around the pandemic recovery agenda. Moreover, shared preoccupations relating to investment in the economy, social inequality, and climate change emerge as major threads shaping budget making. These findings suggest an increasingly integrated multi-level system of economic governance and call for further investigation into the links between ideas shaping EU economic governance and economic discourses in member states. Related Articles Pi Ferrer, Laia, and Pertti Alasuutari. 2019. “The Spread and Domestication of the Term ‘Austerity:’ Evidence from the Portuguese and Spanish Parliaments.” Politics \& Policy 47(6): 1039–65. https://doi.org/10.1111/polp.12331. Zamponi, Lorenzo, and Lorenzo Bosi. 2016. “Which Crisis? European Crisis and National Contexts in Public Discourse.” Politics \& Policy 44(3): 400–26. https://doi.org/10.1111/polp.12156.},
year = {2024}
}
@article{https://doi.org/10.1111/pirs.12550,
author = {Silveira, Douglas and Silva, Izak and Vasconcelos, Silvinha and Perobelli, Fernando},
title = {The Brexit game: uncertainty and location decision},
journal = {Papers in Regional Science},
volume = {99},
number = {6},
pages = {1515-1538},
keywords = {Brexit, firms' location decision, input–output analysis},
doi = {https://doi.org/10.1111/pirs.12550},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pirs.12550},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pirs.12550},
note = { PIRS-FA-2019-2689.R2},
abstract = {Abstract This paper aims to analyse firms' location decisions when faced with Brexit. We combine evolutionary game theory and spatial agent-based simulation approaches with input–output analysis to evaluate two different sectors: (i) crop and animal production; (ii) financial service activities. We separate the European Union in manifold regions and consider the following factors in the decision making: (i) market potential; (ii) productive integration; (iii) labour costs and (iv) displacement cost. Firms assign weights to each of these factors. Our results suggest that in traditional sectors firms tend to seek unsaturated markets. In sectors related to services, the greater the uncertainty, the greater the likelihood that firms will move.},
year = {2020}
}
@article{https://doi.org/10.1155/2019/4324871,
author = {Lamhaddab, Khalid and Lachgar, Mohamed and Elbaamrani, Khalid},
title = {Porting Mobile Apps from iOS to Android: A Practical Experience},
journal = {Mobile Information Systems},
volume = {2019},
number = {1},
pages = {4324871},
doi = {https://doi.org/10.1155/2019/4324871},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2019/4324871},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2019/4324871},
abstract = {The recent rise of smartphones has triggered a revolution in mobile development. As a result of this incremental mobile innovation, new software engineering techniques, software documentation, and tools adapted to the mobile platform remain essential in order to help developers to better understand, analyze, and bootstrap porting mobile applications. In this paper, the authors propose a model-driven reverse-engineering approach based on static analysis, which describes a semantic metamodel of the iOS mobile application and extract design information (such as user interfaces, activity diagram, entities, framework and library dependencies) in order to generate the functional specification documentation and the Android UI skeleton. Thus, aiding the project team, who has in charge porting the app to another mobile platform, to agree upon a consensus on what has to be implemented and safe development cost by auto generating the Android UI skeleton project. To experiment this approach, the authors have implemented a tool called iSpecSnapshot. Moreover, they evaluate the performance of iSpecSnapshot by an experiment involving iOS applications that are ported to Android platform.},
year = {2019}
}

@inbook{doi:https://doi.org/10.1002/9781119861850.ch10,
author = {Raisi, Zobeir and Naiel, Mohamed A. and Younes, Georges and Fieguth, Paul and Zelek, John},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781119861850},
title = {Smart Text Reader System for People who are Blind Using Machine and Deep Learning},
booktitle = {Machine Learning Algorithms for Signal and Image Processing},
chapter = {10},
pages = {161-200},
doi = {https://doi.org/10.1002/9781119861850.ch10},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119861850.ch10},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119861850.ch10},
year = {2022},
keywords = {smart-text reader, blind persons, text detection, text recognition, deep learning, images in the wild},
abstract = {Summary The World Health Organization (WHO) estimates that more than one-sixth of the world's population suffers from some form of visual impairment worldwide. With such a large number of people affected by vision issues, several breakthroughs in technology, both on the hardware and software sides, are needed to make a positive impact and improve the quality of life for blind and sight-impaired people. In particular, the ability to parse text from their surrounding is a crucial aspect that can empower blind people to become active members of society and help them lead an everyday life. Detection and recognition of text in natural images are two significant problems in the field of computer vision, having a wide variety of applications in the analysis of sports videos, autonomous driving, and industrial automation, to name a few, with challenges based on how text is represented and affected by environmental conditions. The current state-of-the-art in text detection and/or recognition has exploited advancements in deep learning and has reported a superior accuracy on benchmark datasets when tackling multi-resolution and multi-oriented text, however, there still remain challenges affecting text in the wild images that cause existing methods to underperform due to models not able to generalize to unseen data from insufficient labeled data. The objectives of this survey chapter are as follows. First, offering the reader not only a review of recent advancements in scene text detection and recognition, but also the results of extensive experiments using a unified evaluation framework that assesses pre-trained models of the selected methods on challenging cases, with the application of consistent evaluation criteria. Second, identifying several existing challenges for detecting or recognizing text in the wild images, namely, in-plane-rotation, multi-oriented and multi-resolution text, perspective distortion, illumination reflection, partial occlusion, complex fonts, and special characters. Finally, the chapter offers insights into potential research directions to address current challenges encountered by scene text detection and recognition techniques.}
}@article{https://doi.org/10.1111/rsp3.12708,
author = {Storti, Luca and Urso, Giulia and Reid, Neil},
title = {Exiting the periphery: Possible pathways towards a socio-economic and institutional de-marginalization of places},
journal = {Regional Science Policy \& Practice},
volume = {15},
number = {7},
pages = {1406-1423},
keywords = {de-marginalization, institutions, peripheries, territorial inequalities},
doi = {https://doi.org/10.1111/rsp3.12708},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rsp3.12708},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/rsp3.12708},
abstract = {Abstract What are the economic, social and institutional mechanisms that make it possible for peripheral areas to regain centrality? To address this issue, this paper takes stock of the current scientific debate on peripheries. Our reasoning is based on a general assumption according to which simple geographic features do not determine per se socio–spatial divisions. Peripheral areas are not natural. By contrast, they are the outcomes of more or less intentional processes that imply hierarchical and power relationships, unequal allocation of economic resources, and reliance upon relational patterns and social norms. Therefore, it is essential to observe how and whether multidimensional socio-economic changes occur within peripheral areas, deconstructing their marginality. In this regard, this paper highlights four main endogenous mechanisms upon which a de-marginalization process may rely: (1) transformation in the features of the local institutional make-up; (2) emergence of collective actions and ‘self-governance’ processes at the local level; (3) renovation of elite groups; and (4) establishing economic renewal and innovation.},
year = {2023}
}
@article{https://doi.org/10.1111/rsp3.12590,
author = {Capello, Roberta and Caragliu, Andrea and Panzera, Elisa},
title = {Economic costs of COVID-19 for cross-border regions},
journal = {Regional Science Policy \& Practice},
volume = {15},
number = {8},
pages = {1688-1701},
keywords = {border effects, costs of COVID19, cross-border regions, regional growth},
doi = {https://doi.org/10.1111/rsp3.12590},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rsp3.12590},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/rsp3.12590},
abstract = {Abstract In Spring 2020, the first wave of the COVID-19 pandemic hit Europe most severely. While empirical evidence regarding the economic costs of the strict lockdown measures enacted during the periods before the widespread diffusion of vaccines is now available, little is known about the economic impact of both strict lockdowns and partial closures on border regions. This is instead a relevant case study to analyze, in particular in the light of the asymmetric nature of border closures. This paper fills this gap and offers two sets of analyses: a first assessment of partial closures, enacted after the first wave of the COVID-19 pandemic and based on the approach applied to European cross-border regions to measure the costs of legal and administrative barriers (Camagni et al., 2019); and a second measurement based on simulating the impacts of full closures with the Macroeconometric Social Sectoral Territorial (MASST)-4 model (Capello \& Caragliu, 2021a). These analyses also allow for the pinpointing of the spatial distribution of economic losses, and to identify whether different regional typologies suffered the highest contraction.},
year = {2023}
}

@inbook{doi:https://doi.org/10.1002/9781119723189.ch2,

publisher = {John Wiley & Sons, Ltd},
isbn = {9781119723189},
title = {The Schema of the Trace, a Paradoxical Semiotics},
booktitle = {The Trace Factory},
chapter = {2},
pages = {31-86},
doi = {https://doi.org/10.1002/9781119723189.ch2},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119723189.ch2},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119723189.ch2},
year = {2020},
keywords = {               Hermes, indexical sign interpretation,                L'homme-trace,                Peircian index theory, scientific publication, trace notion},
abstract = {Summary It is quite disturbing to see how the trace gives rise to different positions. This can be seen by observing the way in which scientific publications construct this object. This chapter presents the study of the frequency of a few terms in French in two collective publications, the issue of the journal Hermes devoted to the theme of “Traceability and Networks” and the collective volume L'homme-trace. This comparison shows to what extent the trace object spreads according to the logic of thought that make use of it. The proposed re-reading of certain aspects of Peircian index theory is based on considering trace as a particular mode of indexical sign interpretation. This partial and simplified commentary was intended to identify some key issues that must be taken into account in order to move towards a more specific examination of the notion of trace.}
}@article{https://doi.org/10.1002/widm.1540,
author = {Chen, Xieling and Xie, Haoran and Tao, Xiaohui and Xu, Lingling and Wang, Jingjing and Dai, Hong-Ning and Wang, Fu Lee},
title = {A topic modeling-based bibliometric exploration of automatic summarization research},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {14},
number = {5},
pages = {e1540},
keywords = {automatic summarization, text mining, topic modeling, trend analysis},
doi = {https://doi.org/10.1002/widm.1540},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1540},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1540},
abstract = {Abstract The surge in text data has driven extensive research into developing diverse automatic summarization approaches to effectively handle vast textual information. There are several reviews on this topic, yet no large-scale analysis based on quantitative approaches has been conducted. To provide a comprehensive overview of the field, this study conducted a bibliometric analysis of 3108 papers published from 2010 to 2022, focusing on automatic summarization research regarding topics and trends, top sources, countries/regions, institutions, researchers, and scientific collaborations. We have identified the following trends. First, the number of papers has experienced 65\% growth, with the majority being published in computer science conferences. Second, Asian countries and institutions, notably China and India, actively engage in this field and demonstrate a strong inclination toward inter-regional international collaboration, contributing to more than 24\% and 20\% of the output, respectively. Third, researchers show a high level of interest in multihead and attention mechanisms, graph-based semantic analysis, and topic modeling and clustering techniques, with each topic having a prevalence of over 10\%. Finally, scholars have been increasingly interested in self-supervised and zero/few-shot learning, multihead and attention mechanisms, and temporal analysis and event detection. This study is valuable when it comes to enhancing scholars' and practitioners' understanding of the current hotspots and future directions in automatic summarization. This article is categorized under: Algorithmic Development > Text Mining},
year = {2024}
}
@article{https://doi.org/10.1111/pirs.12495,
author = {Cuadrado-Roura, Juan R.},
title = {Development, contributions and trends in regional studies in Spain: An overview},
journal = {Papers in Regional Science},
volume = {99},
number = {2},
pages = {327-358},
keywords = {Innovative contributions, Regional research, Spanish contributions, research topics},
doi = {https://doi.org/10.1111/pirs.12495},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pirs.12495},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pirs.12495},
note = { PIRS-FA-2019-2541},
abstract = {Abstract The aim of this paper is to show the development of regional and urban studies in Spain, pointing out the main factors that have influenced such development, the topics analysed by the Spanish researchers and why Spain has today a remarkable position in the field of regional science. Regional problems have always taken up a very important place among the concerns of the Spanish society. This has been closely linked to longstanding historical and internal political conflicts. But, leaving aside some very interesting contributions and territorial projects developed in the past, the research on regional issues emerged really in the 1960s and the consolidation phase took place from 1975 to 2000, as explained in Sections 2 and 3. From the beginning of this century the field has received a clear thrust, supported by a new generation of economists and geographers. The increasing presence of Spanish researchers in international congresses, workshops and specialized journals enable to state that Spain has reached a similar level as in more advanced countries. Section 4 collects the most outstanding topics and innovative contributions made from 2000 to now. A short final note (section 5) points out some final remarks and why a positive continuity of the progress observed can be expected.},
year = {2020}
}
@article{https://doi.org/10.1002/imhj.22038,
author = {Borelli, Jessica L. and Kazmierski, Kelly F. M. and Gaskin, Gerin E. and Kerr, Margaret L. and Smiley, Patricia A. and Rasmussen, Hannah F.},
title = {Savoring interventions for mothers of young children: Mechanisms linking relational savoring and personal savoring to reflective functioning},
journal = {Infant Mental Health Journal: Infancy and Early Childhood},
volume = {44},
number = {2},
pages = {200-217},
keywords = {attachment, intervention, parent-child, reflective functioning, relational savoring, disfrute de la relación, funcionamiento con reflexión, afectividad, progenitor-niño, intervención, Saveur relationnelle, fonctionnement de réflexion, attachement, parent-enfant, intervention, relationales Erleben, reflective Functioning, Bindung, Eltern-Kind, Intervention, 関係性をあじわうrelational savoring、省察機能、愛着、親-子、介入, 关键词:关系品味, 反思功能, 依恋, 亲子, 干预, الكلمات المفتاحية: التذوق العلائقي ، الأداء التأملي ، التعلق ، الوالدين والطفل ، التدخل},
doi = {https://doi.org/10.1002/imhj.22038},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/imhj.22038},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/imhj.22038},
abstract = {Abstract Parenting interventions can improve parenting outcomes, with widespread implications for children's developmental trajectories. Relational savoring (RS) is a brief attachment-based intervention with high potential for dissemination. Here we examine data from a recent intervention trial in order to isolate the mechanisms by which savoring predicts reflective functioning (RF) at treatment follow-up through an examination of the content of savoring sessions (specificity, positivity, connectedness, safe haven/secure base, self-focus, child-focus). Mothers (N = 147, Mage = 30.84 years, SDage = 5.13; Race: 67.3\% White/Caucasian, 12.9\% other or declined to state; 10.9\% biracial/multiracial, 5.4\% Asian, 1.4\% Native American/Alaska Native, 2.0\% Black/African American; Ethnicity: 41.5\% Latina) of toddlers (Mage = 20.96 months, SDage = 2.50; 53.5\% female) were randomized to four sessions of RS or personal savoring (PS). Both RS and PS predicted higher RF, but through different means. RS was indirectly associated with higher RF through greater connectedness and specificity of savoring content, while PS was indirectly associated with higher RF through greater self-focus in savoring content. We discuss the implications of these findings for treatment development and for our understanding of the emotional experience of mothers of toddlers.},
year = {2023}
}
@article{https://doi.org/10.1111/bcpt.13297,

title = {Expression of Concern: Abstracts},
journal = {Basic \& Clinical Pharmacology \& Toxicology},
volume = {125},
number = {S2},
pages = {3-330},
doi = {https://doi.org/10.1111/bcpt.13297},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bcpt.13297},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/bcpt.13297},
year = {2019}
}


