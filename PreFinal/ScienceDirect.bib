@article{SERIES202466,
title = {Can computational models help elucidate the link between complex trauma and hallucinations?},
journal = {Schizophrenia Research},
volume = {265},
pages = {66-73},
year = {2024},
note = {Hallucinations: Neurobiology and Patient Experience},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920996423001834},
author = {Peggy Seriès and Emilie Veerapa and Renaud Jardri},
keywords = {Trauma, Voice hearing, Belief, Inference, Predictive coding, Bayesian models},
abstract = {Recently, a number of predictive coding models have been proposed to account for post-traumatic stress disorder (PTSD)'s symptomatology, including intrusions, flashbacks and hallucinations. These models were usually developed to account for traditional/type-1 PTSD. We here discuss whether these models also apply or can be translated to the case of complex/type-2 PTSD and childhood trauma (cPTSD). The distinction between PTSD and cPTSD is important because the disorders differ in terms of symptomatology and potential mechanisms, how they relate to developmental stages, but also in terms of illness trajectory and treatment. Models of complex trauma could give us insights on hallucinations in physiological/pathological conditions or more generally on the development of intrusive experiences across diagnostic classes.}
}
@article{TSCHANTZ2022108266,
title = {Simulating homeostatic, allostatic and goal-directed forms of interoceptive control using active inference},
journal = {Biological Psychology},
volume = {169},
pages = {108266},
year = {2022},
issn = {0301-0511},
doi = {https://doi.org/10.1016/j.biopsycho.2022.108266},
url = {https://www.sciencedirect.com/science/article/pii/S0301051122000084},
author = {Alexander Tschantz and Laura Barca and Domenico Maisto and Christopher L. Buckley and Anil K. Seth and Giovanni Pezzulo},
keywords = {Interoception, Active inference, Predictive coding, Homeostasis, Allostasis},
abstract = {The adaptive regulation of bodily and interoceptive parameters, such as body temperature, thirst and hunger is a central problem for any biological organism. Here, we present a series of simulations using the framework of active inference to formally characterize interoceptive control and some of its dysfunctions. We start from the premise that the goal of interoceptive control is to minimize a discrepancy between expected and actual interoceptive sensations (i.e., a prediction error or free energy). Importantly, living organisms can achieve this goal by using various forms of interoceptive control: homeostatic, allostatic and goal-directed. We provide a computationally-guided analysis of these different forms of interoceptive control, by showing that they correspond to distinct generative models within Active inference. We discuss how these generative models can support empirical research through enabling fine-grained predictions about physiological and brain signals that may accompany both adaptive and maladaptive interoceptive control.}
}
@article{GUNASEGARAM2021102089,
title = {Towards developing multiscale-multiphysics models and their surrogates for digital twins of metal additive manufacturing},
journal = {Additive Manufacturing},
volume = {46},
pages = {102089},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102089},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421002542},
author = {D.R. Gunasegaram and A.B. Murphy and A. Barnard and T. DebRoy and M.J. Matthews and L. Ladani and D. Gu},
keywords = {Additive manufacturing, Artificial intelligence, Digital twins, Machine learning, Multiscale modeling, Multiphysics modeling, Industry 4.0},
abstract = {Artificial intelligence (AI) embedded within digital models of manufacturing processes can be used to improve process productivity and product quality significantly. The application of such advanced capabilities particularly to highly digitalized processes such as metal additive manufacturing (AM) is likely to make those processes commercially more attractive. AI capabilities will reside within Digital Twins (DTs) which are living virtual replicas of the physical processes. DTs will be empowered to operate autonomously in a diagnostic control capacity to supervise processes and can be interrogated by the practitioner to inform the optimal processing route for any given product. The utility of the information gained from the DTs would depend on the quality of the digital models and, more importantly, their faster-solving surrogates which dwell within DTs for consultation during rapid decision-making. In this article, we point out the exceptional value of DTs in AM and focus on the need to create high-fidelity multiscale-multiphysics models for AM processes to feed the AI capabilities. We identify technical hurdles for their development, including those arising from the multiscale and multiphysics characteristics of the models, the difficulties in linking models of the subprocesses across scales and physics, and the scarcity of experimental data. We discuss the need for creating surrogate models using machine learning approaches for real-time problem-solving. We further identify non-technical barriers, such as the need for standardization and difficulties in collaborating across different types of institutions. We offer potential solutions for all these challenges, after reflecting on and researching discussions held at an international symposium on the subject in 2019. We argue that a collaborative approach can not only help accelerate their development compared with disparate efforts, but also enhance the quality of the models by allowing modular development and linkages that account for interactions between the various sub-processes in AM. A high-level roadmap is suggested for starting such a collaboration.}
}
@article{CRUCITTI2023101076,
title = {Where does the EU cohesion policy produce its benefits? A model analysis of the international spillovers generated by the policy},
journal = {Economic Systems},
volume = {47},
number = {3},
pages = {101076},
year = {2023},
issn = {0939-3625},
doi = {https://doi.org/10.1016/j.ecosys.2023.101076},
url = {https://www.sciencedirect.com/science/article/pii/S0939362523000055},
author = {Francesca Crucitti and Nicholas-Joseph Lazarou and Philippe Monfort and Simone Salotti},
keywords = {Cohesion policy, General equilibrium, Spatial spillovers},
abstract = {In this paper, we investigate the macroeconomic effects of the 2007–2013 cohesion policy investments in the EU. First, we present a detailed overview of the EU budget and the contributions of the Member States for the specific policy under scrutiny. Then, we use a dynamic spatial general equilibrium model to assess the overall impact of the policy both in the short and the long run. Finally, we focus on the spatial spillovers generated by the policy programmes and highlight a number of policy-relevant findings with regard to the debate over the financing of the policy and the divide between its net contributors and net beneficiaries. Our main findings suggest that cohesion policy programmes have had a positive and significant impact on the economies of EU Member States and regions, particularly in the poorest regions of the EU. Spatial spillovers imply that the programmes implemented in the main beneficiaries of the policy also benefit its main contributors. For some of these Member States, spillovers constitute the main source of benefits from cohesion policy.}
}
@article{GIRAY2021111031,
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111031},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111031},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
author = {Görkem Giray},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review},
abstract = {Context:
Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems.
Objective:
The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems.
Method:
I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies.
Results:
The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions.
Conclusion:
The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.}
}
@article{ALEXANDER2023150,
title = {Rethinking retrosplenial cortex: Perspectives and predictions},
journal = {Neuron},
volume = {111},
number = {2},
pages = {150-175},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2022.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0896627322010273},
author = {Andrew S. Alexander and Ryan Place and Michael J. Starrett and Elizabeth R. Chrastil and Douglas A. Nitz},
keywords = {perspective taking, allocentric, egocentric, orientation, episodic memory, temporal sequence, navigation, predictive coding, spatial transformation, network oscillations},
abstract = {Summary
The last decade has produced exciting new ideas about retrosplenial cortex (RSC) and its role in integrating diverse inputs. Here, we review the diversity in forms of spatial and directional tuning of RSC activity, temporal organization of RSC activity, and features of RSC interconnectivity with other brain structures. We find that RSC anatomy and dynamics are more consistent with roles in multiple sensorimotor and cognitive processes than with any isolated function. However, two more generalized categories of function may best characterize roles for RSC in complex cognitive processes: (1) shifting and relating perspectives for spatial cognition and (2) prediction and error correction for current sensory states with internal representations of the environment. Both functions likely take advantage of RSC’s capacity to encode conjunctions among sensory, motor, and spatial mapping information streams. Together, these functions provide the scaffold for intelligent actions, such as navigation, perspective taking, interaction with others, and error detection.}
}
@article{JOHNSON2023100031,
title = {Fairkit, fairkit, on the wall, who’s the fairest of them all? Supporting fairness-related decision-making},
journal = {EURO Journal on Decision Processes},
volume = {11},
pages = {100031},
year = {2023},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2023.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2193943823000043},
author = {Brittany Johnson and Jesse Bartola and Rico Angell and Sam Witty and Stephen Giguere and Yuriy Brun},
keywords = {Software fairness, Algorithmic fairness, Tools for software engineers, Software engineering and artificial intelligence},
abstract = {Modern software relies heavily on data and machine learning, and affects decisions that shape our world. Unfortunately, recent studies have shown that because of biases in data, software systems frequently inject bias into their decisions, from producing more errors when transcribing women’s than men’s voices to overcharging people of color for financial loans. To address bias in software, data scientists and software engineers need tools that help them understand the trade-offs between model quality and fairness in their specific data domains. Toward that end, we present fairkit-learn, an interactive toolkit for helping engineers reason about and understand fairness. Fairkit-learn supports over 70 definition of fairness and works with state-of-the-art machine learning tools, using the same interfaces to ease adoption. It can evaluate thousands of models produced by multiple machine learning algorithms, hyperparameters, and data permutations, and compute and visualize a small Pareto-optimal set of models that describe the optimal trade-offs between fairness and quality. Engineers can then iterate, improving their models and evaluating them using fairkit-learn. We evaluate fairkit-learn via a user study with 54 students, showing that students using fairkit-learn produce models that provide a better balance between fairness and quality than students using scikit-learn and IBM AI Fairness 360 toolkits. With fairkit-learn, users can select models that are up to 67% more fair and 10% more accurate than the models they are likely to train with scikit-learn.}
}
@article{ZAMECNIK2021104499,
title = {Causal and non-causal explanations in code biology},
journal = {Biosystems},
volume = {209},
pages = {104499},
year = {2021},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2021.104499},
url = {https://www.sciencedirect.com/science/article/pii/S0303264721001465},
author = {Lukáš Zámečník},
keywords = {‘Why’-questions, Causal and non-causal explanation, Mechanistic and design explanation, Biological sciences, Code biology},
abstract = {In the philosophy of science, we can consider debates about the nature of non-causal explanations in general (e.g. Reutlinger, Saatsi 2018; Lange 2017) and then especially those in the life sciences (e.g. Huneman, 2018; Kostić 2020). These debates are accompanied by the development of a new mechanism that is becoming the major response to the nature of scientific explanation in the life sciences (e.g. Craver, Darden 2013; Craver 2006); and also by the development of a design explanation (e.g. Eck, Mennes 2016) that represents a modern variant of a functional explanation. In this paper, we will methodically: 1. evaluate the plurality of explanatory strategies in contemporary science (chapter 2). 2. describe the mechanical philosophy and mechanistic explanation (Glennan 2016; Craver, Darden 2013, etc.) (chapter 3). 3. explicate the role of mechanisms in code biology (Barbieri 2015, 2002, etc.) and its relation to the new mechanism (chapter 4). 4. fulfill the main goal of the paper – to apply mechanistic explanations in code biology (Barbieri 2019, etc.) and to apply their suitability for this scientific domain (chapter 5).}
}
@article{RICHARDS2023102502,
title = {Food system labor and bargaining power},
journal = {Food Policy},
volume = {119},
pages = {102502},
year = {2023},
issn = {0306-9192},
doi = {https://doi.org/10.1016/j.foodpol.2023.102502},
url = {https://www.sciencedirect.com/science/article/pii/S0306919223001008},
author = {Timothy J. Richards and Zachariah Rutledge},
keywords = {Bargaining power, Food system labor, Search-and-matching},
abstract = {Historically, pandemics lead to labor shortages, and the COVID-19 pandemic of 2020-21 proved to be no different. While there are many explanations for supply-chain issues reported in a number of industries, the proximate cause for ongoing problems in producing, processing, and delivering food to consumers has been attributed to a lack of labor. If this is the case, then the apparent shortage is likely to manifest in greater bargaining power by workers in the food and agriculture industry, defined generally, during the COVID pandemic. In this paper, we test whether the COVID-19 pandemic is associated with greater bargaining power among food and agriculture workers using a structural model of labor search-and-bargaining, and examine the effect of policy responses to COVID-19 on labor-market outcomes. Using data from the American Community Survey (ACS, Bureau of Census) for wage outcomes in 2019 and 2020, we find that the COVID pandemic was responsible for a 5.7% increase in bargaining power for employed workers. Our counterfactual simulations examine the impact of two labor-market interventions – minimum-wages and unemployment insurance – on equilibrium wages. We find that lower minimum wages leave more employment surplus to employers, allowing them to bid up equilibrium wages, while more generous unemployment insurance reduces the supply of labor, and increases equilibrium wages.}
}
@article{YU20241,
title = {Fuzzing: Progress, Challenges, and Perspectives},
journal = {Computers, Materials and Continua},
volume = {78},
number = {1},
pages = {1-29},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.042361},
url = {https://www.sciencedirect.com/science/article/pii/S154622182400170X},
author = {Zhenhua Yu and Zhengqi Liu and Xuya Cong and Xiaobo Li and Li Yin},
keywords = {Fuzzing, vulnerability, software testing, software security},
abstract = {As one of the most effective techniques for finding software vulnerabilities, fuzzing has become a hot topic in software security. It feeds potentially syntactically or semantically malformed test data to a target program to mine vulnerabilities and crash the system. In recent years, considerable efforts have been dedicated by researchers and practitioners towards improving fuzzing, so there are more and more methods and forms, which make it difficult to have a comprehensive understanding of the technique. This paper conducts a thorough survey of fuzzing, focusing on its general process, classification, common application scenarios, and some state-of-the-art techniques that have been introduced to improve its performance. Finally, this paper puts forward key research challenges and proposes possible future research directions that may provide new insights for researchers.}
}
@article{EASON2024101810,
title = {The importance of highlighting the role of the self in hypnotherapy and hypnosis},
journal = {Complementary Therapies in Clinical Practice},
volume = {54},
pages = {101810},
year = {2024},
issn = {1744-3881},
doi = {https://doi.org/10.1016/j.ctcp.2023.101810},
url = {https://www.sciencedirect.com/science/article/pii/S1744388123000919},
author = {Adam D. Eason and Benjamin A. Parris},
keywords = {Self-hypnosis, Hypnosis, Hypnosis theory, Hypnotherapy},
abstract = {The role of the patient in hypnotherapy can be underestimated by both the therapist and the patient. This is likely due to the focus the hypnosis literature has had on the role played by the hypnotist/therapist and less on the phenomenological control (control over subjective experience) applied by the patient. Whilst early approaches to hypnosis and hypnotherapy included concepts such as autosuggestion and self-hypnosis, the role of the self has been largely overlooked. Here we aim to highlight the importance of the self in hypnotherapy and hypnosis by considering the concept of self-hypnosis and how it relates to hetero-hypnosis. We will show that: 1) historically the self was an important component of the concept of hypnosis; 2) extant theories emphasise the role of the self in hypnosis; 3) self-hypnosis is largely indistinguishable from hetero-hypnosis; 4) self-hypnosis is as effective as hetero-hypnosis. We also argue that highlighting the role of the self in hypnotherapy and hypnosis could increase feelings of self-efficacy, especially given that it can be considered a skill that can be advanced and implies self-control and not “mind-control”. Highlighting the role of phenomenological control by the patient could also increase the uptake of hypnotherapy as treatment for various disorders.}
}
@article{RIBEIRO2023104013,
title = {TEAMSTER: Model-based reinforcement learning for ad hoc teamwork},
journal = {Artificial Intelligence},
volume = {324},
pages = {104013},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.104013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223001595},
author = {João G. Ribeiro and Gonçalo Rodrigues and Alberto Sardinha and Francisco S. Melo},
keywords = {Ad hoc teamwork, Reinforcement learning, Multi-agent systems},
abstract = {This paper investigates the use of model-based reinforcement learning in the context of ad hoc teamwork. We introduce a novel approach, named TEAMSTER, where we propose learning both the environment's model and the model of the teammates' behavior separately. Compared to the state-of-the-art PLASTIC algorithms, our results in four different domains from the multi-agent systems literature show that TEAMSTER is more flexible than the PLASTIC-Model, by learning the environment's model instead of assuming a perfect hand-coded model, and more robust/efficient than PLASTIC-Policy, by being able to continuously adapt to newly encountered teams, without implicitly learning a new environment model from scratch.}
}
@article{LIM201983,
title = {Mal-Flux: Rendering hidden code of packed binary executable},
journal = {Digital Investigation},
volume = {28},
pages = {83-95},
year = {2019},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1742287618303736},
author = {Charles Lim and  Suryadi and Kalamullah Ramli and Yohanes Syailendra Kotualubun},
keywords = {Binary packer, Memory analysis, Malware, Malicious code},
abstract = {A binary packer has commonly been used to protect the original code inside the binary executables from being detected as malicious code by anti-malware software. Various methods of unpacking packed binary executables have been extensively studied, and several unpacking approaches have been proposed. Some of these solutions depend on various assumptions, which may limit their effectiveness. Here, a new method of memory analysis technique, called Mal-Flux, is proposed to determine the end of unpacking routine to allow hidden code extraction from the packed binary executables. Our experiments show that our method provides better performance than previous works in extracting the hidden-code from the packed binary executables.}
}
@article{2020562,
title = {Abstracts},
journal = {Fuel and Energy Abstracts},
volume = {61},
number = {6},
pages = {562-661},
year = {2020},
issn = {0140-6701},
doi = {https://doi.org/10.1016/j.fueleneab.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140670120300345}
}
@article{YANG2024112182,
title = {CfExplainer: Explainable just-in-time defect prediction based on counterfactuals},
journal = {Journal of Systems and Software},
volume = {218},
pages = {112182},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112182},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002267},
author = {Fengyu Yang and Guangdong Zeng and Fa Zhong and Peng Xiao and Wei Zheng and Fuxing Qiu},
keywords = {Explainable just-in-time defect prediction, Counterfactual, Weighted class association rule, Explanation effectiveness, Explanation reliability},
abstract = {Just-in-time (JIT) defect prediction helps rationally allocate testing resources and reduce testing costs. However, most JIT defect prediction models lack explainability, which significantly affects their credibility. Recently, the local interpretable model-agnostic explanations (LIME) method has been used widely in model-explainable research, and many improved LIME-based methods have been proposed. However, problems with respect to explanation effectiveness and reliability remain, which seriously affects the practical use of LIME. To address this problem, CfExplainer, a local rule-based model-agnostic approach, is proposed. The approach first applies counterfactuals to generate synthetic instances. It then mines weighted class association rules based on synthetic instances, and it optimises the process of generating, ranking, pruning, and predicting the class association rules. Next, it employs the rules with the highest priority to explain the prediction results of the model. Experiments were conducted using the public datasets employed in related studies. Compared to other state-of-the-art methods, in terms of explanation effectiveness, CfExplainer's instance similarity improves by 26.5 %-31.2 %, and local model fittness improves by 2.0 %-3.5 %, 2.3 %-3 %, and 0.7 %-7.5 % on the AUC, F1-score, and Popt metrics, respectively. In terms of the reliability of the explanation, explanations that are 2.6 %-4.7 % more unique and 2.5 %-5.9 % more consistent with the actual characteristics of defect-introducing commits than other state-of-the-art methods. Thus, the explanations of the proposed approach can enhance the model credibility and help guide developers in fixing defects and reducing the risk of introducing them.}
}
@article{PENNARTZ2022113969,
title = {What is neurorepresentationalism? From neural activity and predictive processing to multi-level representations and consciousness},
journal = {Behavioural Brain Research},
volume = {432},
pages = {113969},
year = {2022},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2022.113969},
url = {https://www.sciencedirect.com/science/article/pii/S0166432822002376},
author = {Cyriel M.A. Pennartz},
keywords = {Awareness, Deliberation, Evolution, Goal-directed behavior, Intentionality, Model-based learning, Planning, Predictive coding, Situatedness},
abstract = {This review provides an update on Neurorepresentationalism, a theoretical framework that defines conscious experience as multimodal, situational survey and explains its neural basis from brain systems constructing best-guess representations of sensations originating in our environment and body (Pennartz, 2015). It posits that conscious experience is characterized by five essential hallmarks: (i) multimodal richness, (ii) situatedness and immersion, (iii) unity and integration, (iv) dynamics and stability, and (v) intentionality. Consciousness is furthermore proposed to have a biological function, framed by the contrast between reflexes and habits (not requiring consciousness) versus goal-directed, planned behavior (requiring multimodal, situational survey). Conscious experience is therefore understood as a sensorily rich, spatially encompassing representation of body and environment, while we nevertheless have the impression of experiencing external reality directly. Contributions to understanding neural mechanisms underlying consciousness are derived from models for predictive processing, which are trained in an unsupervised manner, do not necessarily require overt action, and have been extended to deep neural networks. Even with predictive processing in place, however, the question remains why this type of neural network activity would give rise to phenomenal experience. Here, I propose to tackle the Hard Problem with the concept of multi-level representations which emergently give rise to multimodal, spatially wide superinferences corresponding to phenomenal experiences. Finally, Neurorepresentationalism is compared to other neural theories of consciousness, and its implications for defining indicators of consciousness in animals, artificial intelligence devices and immobile or unresponsive patients with disorders of consciousness are discussed.}
}
@article{ZAFAR2023105263,
title = {Reviewing methods of deep learning for intelligent healthcare systems in genomics and biomedicine},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105263},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105263},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423006961},
author = {Imran Zafar and Shakila Anwar and Faheem kanwal and Waqas Yousaf and Fakhar {Un Nisa} and Tanzeela Kausar and Qurat {ul Ain} and Ahsanullah Unar and Mohammad Amjad Kamal and Summya Rashid and Khalid Ali Khan and Rohit Sharma},
keywords = {Deep learning, Intelligent healthcare system, Genomics, Bioinformatics, Computational biology, Biobanks, Biomedicine},
abstract = {The advancements in genomics and biomedical technologies have generated vast amounts of biological and physiological data, which present opportunities for understanding human health. Deep learning (DL) and machine learning (ML) are frontiers and interdisciplinary fields of computer science that consider comprehensive computational models and provide integral roles for disease diagnosis and therapy investigation. DL-based algorithms can discover the intrinsic hierarchies in the training data to show great promise for extracting features and learning patterns from complex datasets and performing various analytical tasks. This review comprehensively discusses the wide-ranging DL approaches for intelligent healthcare systems (IHS) in genomics and biomedicine. This paper explores advanced concepts in deep learning (DL) and discusses the workflow of utilizing role-based algorithms in genomics and biomedicine to integrate intelligent healthcare systems (IHS). The aim is to overcome biomedical obstacles like patient disease classification, core biomedical processes, and empowering patient-disease integration. The paper also highlights how DL approaches are well-suited for addressing critical challenges in these domains, offering promising solutions for improved healthcare outcomes. We also provided a concise concept of DL architectures and model optimization in genomics and bioinformatics at the molecular level to deal with biomedicine classification, genomic sequence analysis, protein structure classification, and prediction. Finally, we discussed DL's current challenges and future perspectives in genomics and biomedicine for future directions.}
}
@article{VILLAMAYORTOMAS202048,
title = {Using case study data to understand SES interactions: a model-centered meta-analysis of SES framework applications},
journal = {Current Opinion in Environmental Sustainability},
volume = {44},
pages = {48-57},
year = {2020},
note = {Resilience and complexity:Frameworks and models to capture social-ecological interactions},
issn = {1877-3435},
doi = {https://doi.org/10.1016/j.cosust.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877343520300361},
author = {Sergio Villamayor-Tomas and Christoph Oberlack and Graham Epstein and Stefan Partelow and Matteo Roggero and Elke Kellner and Maurice Tschopp and Michael Cox},
abstract = {Studying social–ecological interactions systematically is difficult when dealing with case study data. The methodological flexibility inherent in case studies facilitates the discovery of complex relationships between social and ecological variables; however, it also poses problems for knowledge accumulation given the diverse ways that variables are measured, as well as the descriptive approaches to causal inference that are typically employed. This article builds on the Social-Ecological Systems Framework (SESF) to systematically compare interactions among variables across case studies. We adopt a ‘model-centered’ meta-analysis of existing SESF case studies, in which the units of analysis are causal explanations including sets of variables and their effects on outcomes (i.e. ‘models’). Our analysis encompassed 30 studies and resulted in the formalization of 125 models. The analysis illustrates opportunities to assess interactions at different levels of detail. The paper concludes by proposing strategies to advance the study and reporting of interactions in case studies to foster a better understanding of dynamics and outcomes of environmental sustainability.}
}
@incollection{NEUSTEIN2022233,
title = {Chapter 9 - Conceptual spaces and scientific data models},
editor = {Amy Neustein and Nathaniel Christen},
booktitle = {Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care},
publisher = {Academic Press},
pages = {233-269},
year = {2022},
isbn = {978-0-323-85197-8},
doi = {https://doi.org/10.1016/B978-0-32-385197-8.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851978000167},
author = {Amy Neustein and Nathaniel Christen},
keywords = {conceptual space theory, syntagmatic graph, role semantics, Quantum , verb-centric grammar, dimensional analysis, emergent syntax, instantiation},
abstract = {This chapter will more substantially develop our approach to Conceptual Space Theory in natural (as well as programming) language contexts, that was initiated in earlier chapters. We present further philosophical motivations for the structural details of our proposed “Syntagmatic Graph” representations and examine techniques for integrating conceptual spaces with linguistic paradigms such as Conceptual Role Semantics and situational semantics. Our central argument is that the classical linguistic concept of “thematic roles” provides an alternative framework for analyzing the semantic integration of multiple conceptual spaces, contrasted with “quantitative blend” models endemic to conceptual space theory proper. Therefore we propose “role-indexed” Conceptual Space models, which have distinct semantic and syntactic patterns, juxtaposing this theory to existing formalizations of conceptual spaces in (for example) Quantum NLP. With that natural-language foundation as a motivation, we then consider semantic models as they could be more concretely applied to scientific data sets.}
}
@article{DORICCHI202256,
title = {Left and right temporal-parietal junctions (TPJs) as “match/mismatch” hedonic machines: A unifying account of TPJ function},
journal = {Physics of Life Reviews},
volume = {42},
pages = {56-92},
year = {2022},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064522000434},
author = {Fabrizio Doricchi and Stefano Lasaponara and Mariella Pazzaglia and Massimo Silvetti},
keywords = {TPJ, Predictive coding, Free energy, Attention, Theory of mind, Sense of agency},
abstract = {Experimental and theoretical studies have tried to gain insights into the involvement of the Temporal Parietal Junction (TPJ) in a broad range of cognitive functions like memory, attention, language, self-agency and theory of mind. Recent investigations have demonstrated the partition of the TPJ in discrete subsectors. Nonetheless, whether these subsectors play different roles or implement an overarching function remains debated. Here, based on a review of available evidence, we propose that the left TPJ codes both matches and mismatches between expected and actual sensory, motor, or cognitive events while the right TPJ codes mismatches. These operations help keeping track of statistical contingencies in personal, environmental, and conceptual space. We show that this hypothesis can account for the participation of the TPJ in disparate cognitive functions, including “humour”, and explain: a) the higher incidence of spatial neglect in right brain damage; b) the different emotional reactions that follow left and right brain damage; c) the hemispheric lateralisation of optimistic bias mechanisms; d) the lateralisation of mechanisms that regulate routine and novelty behaviours. We propose that match and mismatch operations are aimed at approximating “free energy”, in terms of the free energy principle of decision-making. By approximating “free energy”, the match/mismatch TPJ system supports both information seeking to update one's own beliefs and the pleasure of being right in one's own' current choices. This renewed view of the TPJ has relevant clinical implications because the misfunctioning of TPJ-related “match” and “mismatch” circuits in unilateral brain damage can produce low-dimensional deficits of active-inference and predictive coding that can be associated with different neuropsychological disorders.}
}
@article{ATKINS2023103545,
title = {JUE Insight: What is the impact of opportunity zones on job postings?},
journal = {Journal of Urban Economics},
volume = {136},
pages = {103545},
year = {2023},
issn = {0094-1190},
doi = {https://doi.org/10.1016/j.jue.2023.103545},
url = {https://www.sciencedirect.com/science/article/pii/S0094119023000141},
author = {Rachel M.B. Atkins and Pablo Hernández-Lagos and Cristian Jara-Figueroa and Robert Seamans},
keywords = {Opportunity zones, Employment, Place-based policies, Tax policies, Job postings},
abstract = {We study the effect of Opportunity Zones (OZs) on job postings using data comprising the near-universe of U.S. online job postings. The OZs program grants tax breaks for investment in designated distressed communities, nominated by each state’s governor based on multiple factors. We use propensity score matching to account for those factors and a difference-in-differences model over the matched sample to estimate the effect of the program. We find limited evidence of any effect of OZs on job postings on average, although we do find small positive effects in urban areas, in areas with above median Black population, and in some states.}
}
@article{GENG2024103595,
title = {A survey of strategy-driven evasion methods for PE malware: Transformation, concealment, and attack},
journal = {Computers & Security},
volume = {137},
pages = {103595},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823005059},
author = {Jiaxuan Geng and Junfeng Wang and Zhiyang Fang and Yingjie Zhou and Di Wu and Wenhan Ge},
keywords = {Windows portable executable, Malware detection, Malware evasion, Packer, Code obfuscation, Metamorphism, Behavioral obfuscation, Anti-sandbox, Adversarial attack},
abstract = {The continuous proliferation of malware poses a formidable threat to the cyberspace landscape. Researchers have proffered a multitude of sophisticated defense mechanisms aimed at its detection and mitigation. Nevertheless, malware writers persistently pursue pioneering and innovative methods to evade detection by security software, thereby presenting an ever-evolving and dynamic threat to computer systems. Malware evasion refers to the use of certain strategies by malware to evade the detection of security software. Despite numerous surveys on malware evasion techniques, the existing surveys were fragmented and focused on specific types of evasion methods, leading to a lack of systematic and comprehensive research on malware evasion approaches. To fill this gap, this paper proposed a strategy-driven framework from the perspective of malware writers. Based on this framework, we categorize existing evasion detection techniques into transformation (alter the structural and behavioral pattern of the malware), concealment (conceal the behavior of the malware), and attack-based (engage in an attack on the detector to render it inoperable) methods and conduct a comprehensive survey of the relevant research works. In addition, we demonstrate how to integrate existing evasion strategies in the process of generating malware from the perspective of malware writers to subvert the multiple defenses of defenders. Our investigation indicates that: 1) evasion techniques such as packer and code obfuscation remain the foremost selection for attackers, no fewer than 10 off-the-shelf tools provide great assistance to them, 2) environment analysis is the primary concealment-based strategy used by the attacker (48% of the reviewed concealment-based strategy), defenders need greater efforts to counter them, 3) only 3 works discussed techniques for evasion attacks by leveraging fragilities in antivirus engines, meaning that direct attack on the detector is no longer as effective, 4) reinforcement learning algorithm serves as the most popular adversarial attack-based methods and 50% of works based on reinforcement learning are effective against real-world antivirus engines. Furthermore, this paper delves into the development trends in evasive malware and open issues for defenders. The primary objective of this survey is to furnish researchers and practitioners with a thorough comprehension of malware evasion strategies and techniques, thereby fostering the advancement of more potent and efficient approaches to detect and thwart malware.}
}
@article{2021230,
title = {Abstracts},
journal = {Fuel and Energy Abstracts},
volume = {62},
number = {3},
pages = {230-329},
year = {2021},
issn = {0140-6701},
doi = {https://doi.org/10.1016/j.fueleneab.2021.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140670121000102}
}
@article{RABIN2022100429,
title = {ProgramTransformer: A tool for generating semantically equivalent transformed programs},
journal = {Software Impacts},
volume = {14},
pages = {100429},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001130},
author = {Md Rafiqul Islam Rabin and Mohammad Amin Alipour},
keywords = {Program transformation, Code refactoring, Generalizability and robustness, Testing models, Models of code},
abstract = {We propose a tool, referred to as ProgramTransformer, to generate new transformed programs that aim at maintaining the semantics of original programs with different syntactical changes. Given the input and output directories, the tool applies a set of transformations to the methods of programs in the input directory and saves the generated transformed programs into the output directory. The goal is to evaluate the generalizability and robustness of models by observing their predictions on the semantically equivalent transformed programs. Those transformed programs can additionally be useful for improving code representation, data diversity, and counterfactual reasoning of code intelligence models.}
}
@article{SHARMA2024111934,
title = {A survey on machine learning techniques applied to source code},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111934},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111934},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
author = {Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro},
keywords = {Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools},
abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{TAO2024127795,
title = {KFEX-N : A table-text data question-answering model based on knowledge-fusion encoder and EX-N tree decoder},
journal = {Neurocomputing},
volume = {593},
pages = {127795},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127795},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224005666},
author = {Ye Tao and Jiawang Liu and Hui Li and Wenqian Cao and Xiugong Qin and Yunlong Tian and Yongjie Du},
keywords = {Question answering, Tabular and textual, Numerical reasoning, Domain knowledge, Natural language processing},
abstract = {Answering questions about hybrid data combining tables and text is challenging. Recent research has employed encoder-tree decoder frameworks to simulate the reasoning process of arithmetic expressions for generating answers. However, this approach overlooks the inherent diversity of expressions; there might be multiple valid reasoning paths, leading to a decrease in the accuracy of inferred expression trees. Moreover, encoders, lacking rich domain knowledge, struggle to capture deep relationships between questions and supporting evidence; this limitation results in models making errors when selecting operation units. In this paper, we propose a Knowledge-Fusion encoder and EX-N tree decoder table-text data question-answering model(KFEX-N). During the encoding process, the integration of traditional encoders with cross-fusion encoders forms a knowledge-fusion encoder, endowing the model with rich domain knowledge and enhancing its understanding of the operational units required to answer questions. Additionally, we propose an EX-N tree decoder. It reduces the diversity of inference paths through a constrained structure and mitigates the occurrence of final answer errors resulting from decoding errors. We validate our model using publicly available Table-Text QA datasets (TAT-QA and Fin-QA) and achieve state-of-the-art performance.}
}
@article{MAYO2024105513,
title = {Dynamic mutual predictions during social learning: A computational and interbrain model},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {157},
pages = {105513},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105513},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004827},
author = {Oded Mayo and Simone Shamay-Tsoory},
keywords = {Social learning, Interbrain synchrony, Interbrain plasticity, Empathy, Predictive coding, Mutual predictions},
abstract = {During social interactions, we constantly learn about the thoughts, feelings, and personality traits of our interaction partners. Learning in social interactions is critical for bond formation and acquiring knowledge. Importantly, this type of learning is typically bi-directional, as both partners learn about each other simultaneously. Here we review the literature on social learning and propose a new computational and neural model characterizing mutual predictions that take place within and between interactions. According to our model, each partner in the interaction attempts to minimize the prediction error of the self and the interaction partner. In most cases, these inferential models become similar over time, thus enabling mutual understanding to develop. At the neural level, this type of social learning may be supported by interbrain plasticity, defined as a change in interbrain coupling over time in neural networks associated with social learning, among them the mentalizing network, the observation-execution system, and the hippocampus. The mutual prediction model constitutes a promising means of providing empirically verifiable accounts of how relationships develop over time.}
}
@article{PITTS2022107688,
title = {Generalizing the control architecture of the lateral prefrontal cortex},
journal = {Neurobiology of Learning and Memory},
volume = {195},
pages = {107688},
year = {2022},
issn = {1074-7427},
doi = {https://doi.org/10.1016/j.nlm.2022.107688},
url = {https://www.sciencedirect.com/science/article/pii/S1074742722001125},
author = {McKinney Pitts and Derek Evan Nee},
keywords = {Cognitive control, Hierarchy, Prefrontal cortex, fMRI, Dynamic causal modeling},
abstract = {Cognitive control guides non-habitual, goal directed behaviors allowing us to flexibly adapt to ongoing demands. Previous work has suggested that multiple cognitive control processes exist that can be classed according to their action on present-oriented/external information versus future-oriented/internal information. These processes can be mapped onto the lateral prefrontal cortex (LPFC) such that increasingly rostral areas are involved in increasingly future-oriented/internal control processes. Whether and how such processes are organized to support goal-directed behavior remains unclear. On the one hand, the LPFC may flexibly adapt based upon demands. On the other hand, there may be a consistent control architecture such as a control hierarchy that generalizes across demands. Previous work using fMRI in humans during a comprehensive control task that engaged several control processes at once found that an area in mid-LPFC consistently exerted widespread influence throughout the LPFC. These data suggested that the mid-LPFC forms an apex of a putative control hierarchy. However, whether such an architecture generalizes across tasks remains to be tested. Here, we utilized a modified comprehensive control task designed to alter how control processes influence one another to test the generalizability of the LPFC control architecture. Univariate fMRI activations revealed distinct control-related activations relative to past work. Despite these changes, effective connectivity modeling revealed a directed architecture similar to previous findings with the mid-LPFC exerting the most widespread influences throughout LPFC. These results suggest that the fundamental control architecture of the LPFC is relatively fixed, and that different demands are accommodated through modulations of this fixed architecture.}
}
@article{DUONG2020102135,
title = {The impact of payment for forest environmental services (PFES) on community-level forest management in Vietnam},
journal = {Forest Policy and Economics},
volume = {113},
pages = {102135},
year = {2020},
issn = {1389-9341},
doi = {https://doi.org/10.1016/j.forpol.2020.102135},
url = {https://www.sciencedirect.com/science/article/pii/S1389934118302016},
author = {Ngoc T.B. Duong and Wouter T. {De Groot}},
keywords = {Payment for ecosystem services, PES impact, Forest management, Vietnam},
abstract = {In Vietnam, the PFES program of payment for forest environmental services involves more than 100,000 forest right holders. Little is known about the impact of the program on the actual forest management activities by these actors. We used an open-ended self-reporting method in 21 villages in three provinces to characterize changes in forest management due to PFES, to assess levels of change (positive, no change, or negative), and to explore underlying motivations. The major changes identified occurred in the forest patrol groups, trespass and fire prevention, and the effectiveness of forest protection rules. Basically, all respondents reported a positive change in at least one of these features, and no negative change was reported. Respondents also highlighted that the payments and the clarification of boundaries were the main factors underlying this substantial impact. It must be borne in mind however that this success builds on effective forest law enforcement.}
}
@article{SODER2024217,
title = {Continental subduction controls regional magma heterogeneity and distribution of porphyry deposits in post-collisional settings},
journal = {Geochimica et Cosmochimica Acta},
volume = {375},
pages = {217-228},
year = {2024},
issn = {0016-7037},
doi = {https://doi.org/10.1016/j.gca.2024.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0016703724001844},
author = {Christian G. Soder and Jerry Dunga and Rolf L. Romer},
keywords = {Ultrapotassic rocks, Continental subduction, Post-collisional magmatism, Porphyry Cu-Au deposits, Papua New Guinea},
abstract = {Continental subduction is the major cause of regional heterogeneities in the lithospheric mantle and contrasting types of magmatism and mineralization in post-collisional settings. We illustrate the relation between the nature of the subducted crust and the character of magmatism for the Late Miocene New Guinea Orogen that formed by the collision of the Australian continental margin with an island arc. The bipartite nature of the subducted Australian plate margin, with Precambrian crust in the west and Phanerozoic accreted arcs in the east, is reflected in the contrasting magmatism along the strike of the New Guinea Orogen. The chemical signature of the subducted crust is particularly prominent in small-volume Late Miocene–Quaternary ultrapotassic rocks of New Guinea. In the west, ultrapotassic lavas have low εNd values (−12.6 to −20.9), indicating the recycling of ancient continental material. Conversely, high εNd values of +3.5 to +4.5 are found in ultrapotassic lavas from eastern New Guinea. This suggests recycling of juvenile continental material, similar to the orthogneisses exposed in the Late Miocene ultrahigh-pressure metamorphic complex of the D'Entrecasteaux Islands. By comparison with ultrapotassic rocks from other orogenic belts, we show that crustal recycling is responsible for regionally contrasting redox conditions in the lithospheric mantle, which may explain why porphyry-type deposits are important in some regions but absent in others.}
}
@article{SHANKAR2023102507,
title = {Clinical-GAN: Trajectory Forecasting of Clinical Events using Transformer and Generative Adversarial Networks},
journal = {Artificial Intelligence in Medicine},
volume = {138},
pages = {102507},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102507},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723000210},
author = {Vignesh Shankar and Elnaz Yousefi and Alireza Manashty and Dayne Blair and Deepika Teegapuram},
keywords = {Disease trajectory forecasting, Health trajectory, Generative Adversarial Networks, Transformer-based networks, MIMIC-IV},
abstract = {Predicting the trajectory of a disease at an early stage can aid physicians in offering effective treatment, prompt care to patients, and also avoid misdiagnosis. However, forecasting patient trajectories is challenging due to long-range dependencies, irregular intervals between consecutive admissions, and non-stationarity data. To address these challenges, we propose a novel method called Clinical-GAN, a Transformer-based Generative Adversarial Networks (GAN) to forecast the patients’ medical codes for subsequent visits. First, we represent the patients’ medical codes as a time-ordered sequence of tokens akin to language models. Then, a Transformer mechanism is used as a Generator to learn from existing patients’ medical history and is trained adversarially against a Transformer-based Discriminator. We address the above mentioned challenges based on our data modeling and Transformer-based GAN architecture. Additionally, we enable the local interpretation of the model’s prediction using a multi-head attention mechanism. We evaluated our method using a publicly available dataset, Medical Information Mart for Intensive Care IV v1.0 (MIMIC-IV), with more than 500,000 visits completed by around 196,000 adult patients over an 11-year period from 2008–2019. Clinical-GAN significantly outperforms baseline methods and existing works, as demonstrated through various experiments. Source code is at https://github.com/vigi30/Clinical-GAN.}
}
@article{JIANG2024100795,
title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
journal = {Progress in Planning},
volume = {180},
pages = {100795},
year = {2024},
note = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
issn = {0305-9006},
doi = {https://doi.org/10.1016/j.progress.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
keywords = {Generative urban design, Urban form generation, Generative method, AI-generated content (AIGC), Generative AI, Human-machine collaboration},
abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.}
}
@article{HUANG2024121640,
title = {Aligning XAI explanations with software developers’ expectations: A case study with code smell prioritization},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121640},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121640},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423021425},
author = {Zijie Huang and Huiqun Yu and Guisheng Fan and Zhiqing Shao and Mingchen Li and Yuguo Liang},
keywords = {Code smell, Software quality assurance, Explainable artificial intelligence, Empirical software engineering},
abstract = {EXplainable Artificial Intelligence (XAI) aims at improving users’ trust in black-boxed models by explaining their predictions. However, XAI techniques produced unreasonable explanations for software defect prediction since expected outputs (e.g., causes of bugs) were not captured by features used to build models. To set aside feature engineering limitations and evaluate whether XAI could adapt to developers, we exploit XAI for code smell prioritization (i.e., predicting criticalities of sub-optimal coding practices and design choices), whose features could capture developers’ major expectations. We assess the gap between XAI explanations and developers’ expectations in terms of (1) the accuracy of prediction, (2) the coverage of explanations on expectations, and (3) the complexity of explanations. We also narrow the gap by preserving the features related to developers’ expectations as much as possible in feature selection. We find that XAI can explain smells with simpler causes in top 3 to 5 features. Complex smells can be explained in around 10 features, which need more expertise to interpret. Selecting features adapting to the developers’ expectations improves coverage by 5% to 29%, with almost no negative impact on accuracy and complexity. Results also highlight the need of dividing coarse-grained prediction targets and developing fine-grained feature engineering.}
}
@article{BALERA2019176,
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
journal = {Information and Software Technology},
volume = {114},
pages = {176-189},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301430},
author = {Juliana Marino Balera and Valdivino Alexandre de {Santiago Júnior}},
keywords = {Search-based Software Testing, Hyper-heuristics, Systematic Mapping, Evolutionary Algorithms, Genetic Algorithms, Meta-heuristics},
abstract = {Context
Search-based Software Testing (SBST) is a research field where testing a software product is formulated as an optimization problem. It is an active sub-area of Search-based Software Engineering (SBSE) where many studies have been published and some reviews have been carried out. The majority of studies in SBST has been adopted meta-heuristics while hyper-heuristics have a long way to go. Moreover, there is still a lack of studies to perceive the state-of-the-art of the use of hyper-heuristics within SBST.
Objective
The objective of this work is to investigate the adoption of hyper-heuristics for Software Testing highlighting the current efforts and identifying new research directions.
Method
A Systematic mapping study was carried out with 5 research questions considering papers published up to may/2019, and 4 different bases. The research questions aims to find out, among other things, what are the hyper-heuristics used in the context of Software Testing, for what problems hyper-heuristics have been applied, and what are the objective functions in the scope of Software Testing.
Results
A total of 734 studies were found via the search strings and 164 articles were related to Software Testing. However, from these, only 26 papers were actually in accordance with the scope of this research and 3 more papers were considered due to snowballing or expert’s suggestion, totalizing 29 selected papers. Few different problems and application domains where hyper-heuristics have been considered were identified.
Conclusion
Differently from other communities (Operational Research, Artificial Intelligence), SBST has little explored the benefits of hyper-heuristics which include generalization and less difficulty in parameterization. Hence, it is important to further investigate this area in order to alleviate the effort of practitioners to use such an approach in their testing activities.}
}
@article{LI2021102371,
title = {I-MAD: Interpretable malware detector using Galaxy Transformer},
journal = {Computers & Security},
volume = {108},
pages = {102371},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102371},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821001954},
author = {Miles Q. Li and Benjamin C.M. Fung and Philippe Charland and Steven H.H. Ding},
keywords = {Cybersecurity, Malware detection, Deep learning, Transformers, Interpretability},
abstract = {Malware currently presents a number of serious threats to computer users. Signature-based malware detection methods are limited in detecting new malware samples that are significantly different from known ones. Therefore, machine learning-based methods have been proposed, but there are two challenges these methods face. The first is to model the full semantics behind the assembly code of malware. The second challenge is to provide interpretable results while keeping excellent detection performance. In this paper, we propose an Interpretable MAlware Detector (I-MAD) that outperforms state-of-the-art static malware detection models regarding accuracy with excellent interpretability. To improve the detection performance, I-MAD incorporates a novel network component called the Galaxy Transformer network that can understand assembly code at the basic block, function, and executable levels. It also incorporates our proposed interpretable feed-forward neural network to provide interpretations for its detection results by quantifying the impact of each feature with respect to the prediction. Experiment results show that our model significantly outperforms existing state-of-the-art static malware detection models and presents meaningful interpretations.}
}
@article{UCCI2019123,
title = {Survey of machine learning techniques for malware analysis},
journal = {Computers & Security},
volume = {81},
pages = {123-147},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818303808},
author = {Daniele Ucci and Leonardo Aniello and Roberto Baldoni},
keywords = {Portable executable, Malware analysis, Machine learning, Benchmark, Malware analysis economics},
abstract = {Coping with malware is getting more and more challenging, given their relentless growth in complexity and volume. One of the most common approaches in literature is using machine learning techniques, to automatically learn models and patterns behind such complexity, and to develop technologies to keep pace with malware evolution. This survey aims at providing an overview on the way machine learning has been used so far in the context of malware analysis in Windows environments, i.e. for the analysis of Portable Executables. We systematize surveyed papers according to their objectives (i.e., the expected output), what information about malware they specifically use (i.e., the features), and what machine learning techniques they employ (i.e., what algorithm is used to process the input and produce the output). We also outline a number of issues and challenges, including those concerning the used datasets, and identify the main current topical trends and how to possibly advance them. In particular, we introduce the novel concept of malware analysis economics, regarding the study of existing trade-offs among key metrics, such as analysis accuracy and economical costs.}
}
@article{AJORLOO2024111805,
title = {A systematic review of machine learning methods in software testing},
journal = {Applied Soft Computing},
volume = {162},
pages = {111805},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111805},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624005799},
author = {Sedighe Ajorloo and Amirhossein Jamarani and Mehdi Kashfi and Mostafa {Haghi Kashani} and Abbas Najafizadeh},
keywords = {Machine learning, Software testing, Quality of software, Systematic review},
abstract = {Background
The quest for higher software quality remains a paramount concern in software testing, prompting a shift towards leveraging machine learning techniques for enhanced testing efficacy.
Objective
The objective of this paper is to identify, categorize, and systematically compare the present studies on software testing utilizing machine learning methods.
Method
This study conducts a systematic literature review (SLR) of 40 pertinent studies spanning from 2018 to March 2024 to comprehensively analyze and classify machine learning methods in software testing. The review encompasses supervised learning, unsupervised learning, reinforcement learning, and hybrid learning approaches.
Results
The strengths and weaknesses of each reviewed paper are dissected in this study. This paper also provides an in-depth analysis of the merits of machine learning methods in the context of software testing and addresses current unresolved issues. Potential areas for future research have been discussed, and statistics of each review paper have been collected.
Conclusion
By addressing these aspects, this study contributes to advancing the discourse on machine learning's role in software testing and paves the way for substantial improvements in testing efficacy and software quality.}
}
@article{RAJAN2024112090,
title = {Distribution-aware fairness test generation},
journal = {Journal of Systems and Software},
volume = {215},
pages = {112090},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112090},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001353},
author = {Sai Sathiesh Rajan and Ezekiel Soremekun and Yves {Le Traon} and Sudipta Chattopadhyay},
keywords = {Software testing, Fairness testing, Computer vision},
abstract = {Ensuring that all classes of objects are detected with equal accuracy is essential in AI systems. For instance, being unable to identify any one class of objects could have fatal consequences in autonomous driving systems. Hence, ensuring the reliability of image recognition systems is crucial. This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DISTROFAIR) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DISTROFAIR automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations – object deletion, object insertion and object rotation. We evaluate DISTROFAIR using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DISTROFAIR reveal class-level fairness violations using either ground truth or metamorphic oracles. DISTROFAIR is up to 2.3× more effective than two main baselines, i.e., (a) an approach which focuses on generating images only within the distribution (ID) and (b) fairness analysis using only the original image dataset. We further observed that DISTROFAIR is efficient, it generates 460 images per hour, on average. Finally, we evaluate the semantic validity of our approach via a user study with 81 participants, using 30 real images and 30 corresponding mutated images generated by DISTROFAIR. We found that images generated by DISTROFAIR are 80% as realistic as real-world images.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}
@article{SHARMA2024111934,
title = {A survey on machine learning techniques applied to source code},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111934},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111934},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
author = {Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro},
keywords = {Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools},
abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{ZHOU20242069,
title = {C-CORE: Clustering by Code Representation to Prioritize Test Cases in Compiler Testing},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {2},
pages = {2069-2093},
year = {2024},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.043248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149224001930},
author = {Wei Zhou and Xincong Jiang and Chuan Qin},
keywords = {Compiler testing, test case prioritization, code representation},
abstract = {Edge devices, due to their limited computational and storage resources, often require the use of compilers for program optimization. Therefore, ensuring the security and reliability of these compilers is of paramount importance in the emerging field of edge AI. One widely used testing method for this purpose is fuzz testing, which detects bugs by inputting random test cases into the target program. However, this process consumes significant time and resources. To improve the efficiency of compiler fuzz testing, it is common practice to utilize test case prioritization techniques. Some researchers use machine learning to predict the code coverage of test cases, aiming to maximize the test capability for the target compiler by increasing the overall predicted coverage of the test cases. Nevertheless, these methods can only forecast the code coverage of the compiler at a specific optimization level, potentially missing many optimization-related bugs. In this paper, we introduce C-CORE (short for Clustering by Code Representation), the first framework to prioritize test cases according to their code representations, which are derived directly from the source codes. This approach avoids being limited to specific compiler states and extends to a broader range of compiler bugs. Specifically, we first train a scaled pre-trained programming language model to capture as many common features as possible from the test cases generated by a fuzzer. Using this pre-trained model, we then train two downstream models: one for predicting the likelihood of triggering a bug and another for identifying code representations associated with bugs. Subsequently, we cluster the test cases according to their code representations and select the highest-scoring test case from each cluster as the high-quality test case. This reduction in redundant testing cases leads to time savings. Comprehensive evaluation results reveal that code representations are better at distinguishing test capabilities, and C-CORE significantly enhances testing efficiency. Across four datasets, C-CORE increases the average of the percentage of faults detected (APFD) value by 0.16 to 0.31 and reduces test time by over 50% in 46% of cases. When compared to the best results from approaches using predicted code coverage, C-CORE improves the APFD value by 1.1% to 12.3% and achieves an overall time-saving of 159.1%.}
}
@article{RABIN2022100429,
title = {ProgramTransformer: A tool for generating semantically equivalent transformed programs},
journal = {Software Impacts},
volume = {14},
pages = {100429},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001130},
author = {Md Rafiqul Islam Rabin and Mohammad Amin Alipour},
keywords = {Program transformation, Code refactoring, Generalizability and robustness, Testing models, Models of code},
abstract = {We propose a tool, referred to as ProgramTransformer, to generate new transformed programs that aim at maintaining the semantics of original programs with different syntactical changes. Given the input and output directories, the tool applies a set of transformations to the methods of programs in the input directory and saves the generated transformed programs into the output directory. The goal is to evaluate the generalizability and robustness of models by observing their predictions on the semantically equivalent transformed programs. Those transformed programs can additionally be useful for improving code representation, data diversity, and counterfactual reasoning of code intelligence models.}
}
@article{RABIN2021106552,
title = {On the generalizability of Neural Program Models with respect to semantic-preserving program transformations},
journal = {Information and Software Technology},
volume = {135},
pages = {106552},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106552},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000379},
author = {Md Rafiqul Islam Rabin and Nghi D.Q. Bui and Ke Wang and Yijun Yu and Lingxiao Jiang and Mohammad Amin Alipour},
keywords = {Neural models, Code representation, Model evaluation, Program transformation, Generalizability},
abstract = {Context:
With the prevalence of publicly available source code repositories to train deep neural network models, neural program models can do well in source code analysis tasks such as predicting method names in given programs that cannot be easily done by traditional program analysis techniques. Although such neural program models have been tested on various existing datasets, the extent to which they generalize to unforeseen source code is largely unknown.
Objective:
Since it is very challenging to test neural program models on all unforeseen programs, in this paper, we propose to evaluate the generalizability of neural program models with respect to semantic-preserving transformations: a generalizable neural program model should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures.
Method:
We compare the results of various neural program models for the method name prediction task on programs before and after automated semantic-preserving transformations. We use three Java datasets of different sizes and three state-of-the-art neural network models for code, namely code2vec, code2seq, and GGNN, to build nine such neural program models for evaluation.
Results:
Our results show that even with small semantically preserving changes to the programs, these neural program models often fail to generalize their performance. Our results also suggest that neural program models based on data and control dependencies in programs generalize better than neural program models based only on abstract syntax trees (ASTs). On the positive side, we observe that as the size of the training dataset grows and diversifies the generalizability of correct predictions produced by the neural program models can be improved too.
Conclusion:
Our results on the generalizability of neural program models provide insights to measure their limitations and provide a stepping stone for their improvement.}
}
@article{SHARMA2024111934,
title = {A survey on machine learning techniques applied to source code},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111934},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111934},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
author = {Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro},
keywords = {Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools},
abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{WIJEKOON2023110830,
title = {A user-centred evaluation of DisCERN: Discovering counterfactuals for code vulnerability detection and correction},
journal = {Knowledge-Based Systems},
volume = {278},
pages = {110830},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110830},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123005804},
author = {Anjana Wijekoon and Nirmalie Wiratunga},
keywords = {Counterfactual explanations, Vulnerability detection, Explainable AI},
abstract = {Counterfactual explanations highlight actionable knowledge which helps to understand how a machine learning model outcome could be altered to a more favourable outcome. Understanding actionable corrections in source code analysis can be critical to proactively mitigate security attacks that are caused by known vulnerabilities. In this paper, we present the DisCERN explainer for discovering counterfactuals for code vulnerability correction. Given a vulnerable code segment, DisCERN finds counterfactual (i.e. non-vulnerable) code segments and recommends actionable corrections. DisCERN uses feature attribution knowledge to identify potentially vulnerable code statements. Subsequently, it applies a substitution-focused correction, suggesting suitable fixes by analysing the nearest-unlike neighbour. Overall, DisCERN aims to identify vulnerabilities and correct them while preserving both the code syntax and the original functionality of the code. A user study evaluated the utility of counterfactuals for vulnerability detection and correction compared to more commonly used feature attribution explainers. The study revealed that counterfactuals foster positive shifts in mental models, effectively guiding users towards making vulnerability corrections. Furthermore, counterfactuals significantly reduced the cognitive load when detecting and correcting vulnerabilities in complex code segments. Despite these benefits, the user study showed that feature attribution explanations are still more widely accepted than counterfactuals, possibly due to the greater familiarity with the former and the novelty of the latter. These findings encourage further research and development into counterfactual explanations, as they demonstrate the potential for acceptability over time among developers as a reliable resource for both coding and training.}
}
@article{LI2025130105,
title = {LLM-guided decision-making toolkit for multi-agent reinforcement learning},
journal = {Neurocomputing},
volume = {638},
pages = {130105},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130105},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225007775},
author = {Zhemin Li and Ruobing Zhang and Zhengming Wang and Zheng Xie and Yiping Song},
keywords = {MARL, LLM, SMAC, Cold-start problem},
abstract = {Complex cooperative game tasks pose significant challenges to decision making in multi-agent systems. Despite advancements in multi-agent reinforcement learning (MARL), these systems often struggle in the initial learning phase, known as the cold-start problem, where devising effective strategies from scratch is difficult. To address this, we leverage the advanced reasoning capabilities of Large Language Models (LLMs) by introducing prompt learning in complex cooperative games. In our approach, tensor-formatted environmental information is converted into natural language and fed into LLMs during early exploration stages in MARL algorithms. The strategic suggestions generated by LLMs, combined with trajectories from random exploration, form an experience pool for initial training. Our experiments demonstrate that this approach significantly enhances cooperation among agents in complex games and mitigates the cold-start issue. Notably, we have developed a toolkit compatible with the StarCraft Multi-Agent Challenge (SMAC) environment, providing seamless integration between SMAC APIs, LLMs, and MARL algorithms. This includes automated prompt generation for any map and alignment of natural language strategies with tensor-formatted data. In addition, considering the importance of effective communication for enhancing collaboration efficiency, our toolkit includes a robust communication module, which supports the training, testing, and visualization of communication algorithms. We provide the source code on GitHub at https://github.com/lizhemin18/pymarl_LLM.}
}
@article{ALAM2025100232,
title = {A Review of the Applications, Benefits, and Challenges of Generative AI for Sustainable Toxicology},
journal = {Current Research in Toxicology},
volume = {8},
pages = {100232},
year = {2025},
issn = {2666-027X},
doi = {https://doi.org/10.1016/j.crtox.2025.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2666027X25000180},
author = {Furqan Alam and Tahani Saleh {Mohammed Alnazzawi} and Rashid Mehmood and Ahmed Al-maghthawi},
keywords = {Generative AI (GenAI), Artificial Intelligence, Toxicology, Drug Discovery, Conversational-AI, Toxicity Prediction, Risk Assessment, Risk Management},
abstract = {Sustainable toxicology is vital for living species and the environment because it guarantees the safety, efficacy, and regulatory compliance of drugs, treatments, vaccines, and chemicals in living organisms and the environment. Conventional toxicological methods often lack sustainability as they are costly, time-consuming, and sometimes inaccurate. It means delays in producing new drugs, vaccines, and treatments and understanding the adverse effects of the chemicals on the environment. To address these challenges, the healthcare sector must leverage the power of the Generative-AI (GenAI) paradigm. This paper aims to help understand how the healthcare field can be revolutionized in multiple ways by using GenAI to facilitate sustainable toxicological developments. This paper first reviews the present literature and identifies the possible classes of GenAI that can be applied to toxicology. A generalized and holistic visualization of various toxicological processes powered by GenAI is presented in tandem. The paper discussed toxicological risk assessment and management, spotlighting how global agencies and organizations are forming policies to standardize and regulate AI-related development, such as GenAI, in these fields. The paper identifies and discusses the advantages and challenges of GenAI in toxicology. Further, the paper outlines how GenAI empowers Conversational-AI, which will be critical for highly tailored toxicological solutions. This review will help to develop a comprehensive understanding of the impacts and future potential of GenAI in the field of toxicology. The knowledge gained can be applied to create sustainable GenAI applications for various problems in toxicology, ultimately benefiting our societies and the environment.}
}
@article{YANG2025113503,
title = {A comprehensive survey on integrating large language models with knowledge-based methods},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113503},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113503},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005490},
author = {Wenli Yang and Lilian Some and Michael Bain and Byeong Kang},
keywords = {LLMs, Knowledge-based, Knowledge integration, RAG, KG},
abstract = {The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.}
}
@article{CHOWDHURY2025112827,
title = {R-VQA: A robust visual question answering model},
journal = {Knowledge-Based Systems},
volume = {309},
pages = {112827},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112827},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124014618},
author = {Souvik Chowdhury and Badal Soni},
keywords = {Visual question answering, Large language model, In-context learning, Computer vision, Natural language processing, Generative artificial intelligence},
abstract = {Visual Question Answering (VQA) involves generating answers to questions about visual content, such as images. VQA models process an image and a question to produce an answer. One major challenge in this domain is robustness, as current VQA models often operate within a fixed answer space and struggle with issues related to language prior (favoring frequent answers) and compositional reasoning (difficulty with complex object relationships). While existing research addresses these challenges separately, no work has tackled both language prior and compositional reasoning simultaneously. This paper presents three key contributions: the development of a dataset specifically designed to address language prior and compositional reasoning issues, the creation of a unified model capable of addressing both problems in a single inference, and the ability to generate answers beyond a predefined answer space. Our proposed model, R-VQA, demonstrates superior performance compared to state-of-the-art (SOTA) models across various VQA datasets.}
}
@article{WANG2025103060,
title = {Emotion inference of text based on counterfactual behavior knowledge},
journal = {Information Fusion},
volume = {120},
pages = {103060},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103060},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001332},
author = {Xinzhi Wang and Jiayan Qian and Yudong Chang and Hang Yu and Hui Zhang},
keywords = {Text mining, Emotion inference, Knowledge fusion, Affective computing, Counterfactual behavior knowledge},
abstract = {Reader’s emotion is triggered by writer’s expression and content of text. Reader’s emotion inference can help industries and companies discover the preferences and needs of readers and customize appealing content. Currently, most scholars focus on mining emotion expressed by writers, while neglecting easily-transferred reader’s emotion which is simultaneously influenced by objective event content, subjective writer’s affect and individual cognition. Faced with that, we propose a reader’s emotion inference method based on counterfactual behavior knowledge. Multi-granularity elements are extracted, including event indicator, behavior driver and subjective words that hidden in text. Three steps are included in this method. First, counterfactual behavior knowledge is constructed by replacing A–F–B knowledge in original text, which assists models to process and understand the relationships between events and emotions according to the fusion of facts and counterfacts. Second, a knowledge prompt method is proposed, which splice the A–F–B knowledge (as a new feature) to supplement and reinforce established facts. Third, a decision enhancement method is proposed, which employs self-reflection mechanism to fuse original decision and improved decision based on emotion transfer preferences. Experiments and a questionnaire survey are conducted on social news data with reader’s emotion votes. The results show that the models with proposed method outperforms baseline models.}
}
@article{CHAN2025112330,
title = {Effectiveness of symmetric metamorphic relations on validating the stability of code generation LLM},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112330},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112330},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003741},
author = {Pak Yuen Patrick Chan and Jacky Keung and Zhen Yang},
keywords = {Metamorphic testing, Metamorphic relation, True satisfaction, Large language model, Code generation},
abstract = {Pre-trained large language models (LLMs) are increasingly used in software development for code generation, with a preference for private LLMs over public ones to avoid the risk of exposing corporate secrets. Validating the stability of these LLMs’ outputs is crucial, and our study proposes using symmetric Metamorphic Relations (MRs) from Metamorphic Testing (MT) for this purpose. Our study involved an empirical experiment with ten LLMs (eight private and two public) and two publicly available datasets. We defined seven symmetric MRs to generate “Follow-up” datasets from “Source” datasets for testing. Our evaluation aimed to detect violations (inconsistent predictions) between “Source” and “Follow-up” datasets and assess the effectiveness of MRs in identifying correct and incorrect non-violated predictions from ground truths. Results showed that one public and four private LLMs did not violate “Case transformation of prompts” MR. Furthermore, effectiveness and performance results indicated that proposed MRs are effective tools for explaining the instability of LLM's outputs by “Case transformation of prompts”, “Duplication of prompts”, and “Paraphrasing of prompts”. The study underscored the importance of enhancing LLMs’ semantic understanding of prompts for better stability and highlighted potential future research directions, including exploring different MRs, enhancing semantic understanding, and applying symmetry to prompt engineering.}
}
@article{SHARMA2024111934,
title = {A survey on machine learning techniques applied to source code},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111934},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111934},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
author = {Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro},
keywords = {Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools},
abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{WANG2025112387,
title = {Improving user-oriented fairness in recommendation via data augmentation: Don’t worry about inactive users},
journal = {Journal of Systems and Software},
volume = {225},
pages = {112387},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112387},
url = {https://www.sciencedirect.com/science/article/pii/S016412122500055X},
author = {Yong Wang and Huadong Zhou and Gui-Fu Lu and Cuiyun Gao and Shuai Meng},
keywords = {Data augmentation, Fair recommendation, Group fairness, Large Language Models},
abstract = {A recommendation system is considered unfair when it does not perform equally well for different user groups according to users’ specific attributes. In recent research, the user groups are divided into active user group and inactive user group according to the number of interaction records in a recommendation system. Intuitively, increasing the number of inactive users’ interaction records would improve the fairness of the recommendation system. Existing data augmentation techniques can increase interaction records, however they usually fail to deeply mine user interaction patterns and fail to generate context-related feedback, which cannot effectively improve the quality of recommendations for inactive users. To resolve the problem, we use the Large Language Models (LLMs) to mine user historical interaction records to achieve data augmentation, which improve the quality of recommendations for inactive user groups. Experimental results on four classic baseline recommendation algorithms show that our data augmentation method for the inactive user group can effectively alleviate the poor recommendation quality caused by the low interaction with the recommendation system, reduce the recommendation quality gap with active user group, and further improve the user group fairness of the recommendation system.}
}
@article{WEN2025104169,
title = {FGVIrony: A Chinese Dataset of Fine-grained Verbal Irony},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104169},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104169},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001104},
author = {Zhiyuan Wen and Rui Wang and Qianlong Wang and Lin Gui and Yunfei Long and Shiwei Chen and Bin Liang and Min Yang and Ruifeng Xu},
keywords = {Verbal irony detection, Fine-grained verbal irony recognition, Language resources, Prompt learning, Natural language processing},
abstract = {Verbal irony, identified as an incongruity between a speaker’s intended meaning and their explicit linguistic expression, often manifests in nuanced forms such as irony, sarcasm, and satire. Current research often fails to differentiate among these fine-grained categories of verbal irony, primarily focusing on generic detection in texts. Therefore, in this work, we introduce a new task for fine-grained verbal irony recognition, aims not only to identify the presence of verbal irony but also distinguish among its various types. Besides, a notable gap in existing research is the lack of datasets tailored to fine-grained verbal irony, particularly in the context of the Chinese language. To tackle this issue, we have developed the FGVIrony dataset, which comprises 10,252 samples, including 6,790 non-ironic and 3,462 verbal ironic instances, further classified into 1,796 instances of irony, 362 of sarcasm, 577 of satire, 192 overstatements, 79 understatements, and 456 rhetorical questions. On the FGVIrony dataset, we explore the challenges of accurately identifying fine-grained verbal irony. Additionally, to investigate the limitations inherent in current methodologies, we propose a cascaded multi-prompt learning approach, CMP, designed to enhance recognition accuracy. The FGVIrony dataset is available at .}
}
